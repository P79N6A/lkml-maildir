Date: Fri, 14 Mar 2008 10:36:52 -0700
From: "Yinghai Lu" <>
Subject: Re: [PATCH] mm: fix boundary checking in free_bootmem_core
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2008/3/14/202

On Fri, Mar 14, 2008 at 9:53 AM, Andi Kleen <andi@firstfloor.org> wrote:
> On Fri, Mar 14, 2008 at 09:44:50AM -0700, Yinghai Lu wrote:
>  > On 14 Mar 2008 12:58:44 +0100, Andi Kleen <andi@firstfloor.org> wrote:
>  > > "Yinghai Lu" <yhlu.kernel@gmail.com> writes:
>  > >  >
>  > >  > then i tried to reserve 64M or 128M RAM before that, and free that
>  > >  > before gart/switotble try to allloc_bootmem under 4g.
>  > >
>  > >  Sounds like an incredible hack. There are far better ways to do that
>  > >  for bootmem allocations. e.g. you can just specify a high enough "goal"
>  > >  That is how swiotlb solves a similar problem (at least before my
>  > >  mask allocator rewrite)
>  >
>  > I don't think so.
>  >
>  > anyway, otherway to workaround it is
>  > change
>  >                 return __earlyonly_bootmem_alloc(node, size, size,
>  >                                 __pa(MAX_DMA_ADDRESS));
>  > in vmemmap_alloc_block to
>  >                 return __earlyonly_bootmem_alloc(node, size, size,
>  >                                 __pa(MAX_DMA_ADDRESS + (1<<27)));
>  > to make room for gart. but that is global change. and may affect other
>  > platform.
>
>  You can just make it an optional architecture defined macro
it is hard to use MACRO, if someone have allsysconfig, that will make
kernel code use a lot.
>
>
>  > and don't make sure gart will get it.
>
>  Has nothing to do with the gart?
>
>
>
>  >
>  > also i assume swiotlb need that range is less than 4g.
>
>  The normal rule is that anybody who needs big bootmem allocations
>  need to make sure they're high enough to not fill up first 4GB.
>  For small allocations like most of bootmem it doesn't matter because
>  they're, um, small.
>
>  If vmemmap doesn't do that vmemmap needs to be fixed.
how to define big?
it has hundreds of 2M block. when numa is on, they span on all nodes,
and if numa is off, they are sitting on first_online_node.
YH