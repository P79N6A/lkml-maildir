Date: 25 Jan 2000 08:40:24 -0800
From: (H. Peter Anvin)
Subject: Re: SMP Theory (was: Re: Interesting analysis of linux kernelthreading 	by IBM)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2000/1/25/190

Followup to:  <388DAF02.1EB20B6D@idb.hist.no>
By author:    Helge Hafting <helgehaf@idb.hist.no>
In newsgroup: linux.dev.kernel
>
> > In a SMP box all CPU's have equal access to all memory. This does cause
> > the exact problems that you describe. One solution to this is to give each
> > CPU it's own memory, the problem then is that some parts of memory are
> > easier to get at then others, depending on which CPU you are accessing
> > them from. thus Non Uniform Memory Access machines (NUMA).
> 
> Another solution is to avoid the per-cpu caches.  Yes - one big cache
> for all processors.  I believe this is hard to achieve on a circuit
> board,
> those low-latency caches need impressive signalling speed incompatible
> with ten-centimeter distances.    But it may be a good solution
> for a dual/quad-on-a-chip cpu.
> 
No, it's not.  On-chip L2 caches exist because you can't build a
larger L1 cache anymore.  Although you may be able to share the L2
cache on a chip (in fact, you probably can), sharing the L1 cache is
out of the question.  You can't even share the L1 cache between the
ifetch unit and the memory unit anymore... hence separate L1 I- and
D-caches.
	-hpa
-- 
<hpa@transmeta.com> at work, <hpa@zytor.com> in private!
"Unix gives you enough rope to shoot yourself in the foot."
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.rutgers.edu
Please read the FAQ at 
http://www.tux.org/lkml/