Date: Tue, 21 Nov 2006 16:59:05 +0100
From: Duncan Sands <>
Subject: [PATCH] fix asm constraints in i386 atomic_add_return
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2006/11/21/125

Since v->counter is both read and written, it should be an
output as well as an input for the asm.  The current code
only gets away with this because counter is volatile.  Also,
according to Documents/atomic_ops.txt, atomic_add_return
should provide a memory barrier, in particular a compiler
barrier, so the asm should be marked as clobbering memory.
Test case attached.
Signed-off-by: Duncan Sands <baldrick@free.fr>
diff --git a/include/asm-i386/atomic.h b/include/asm-i386/atomic.h
index 51a1662..6aab7a1 100644
--- a/include/asm-i386/atomic.h
+++ b/include/asm-i386/atomic.h
@@ -187,9 +187,9 @@ static __inline__ int atomic_add_return(
 	/* Modern 486+ processor */
 	__i = i;
 	__asm__ __volatile__(
-		LOCK_PREFIX "xaddl %0, %1;"
-		:"=r"(i)
-		:"m"(v->counter), "0"(i));
+		LOCK_PREFIX "xaddl %0, %1"
+		:"+r" (i), "+m" (v->counter)
+		: : "memory");
 	return i + __i;
 
 #ifdef CONFIG_M386#include <stdio.h>
typedef struct { int counter; } atomic_t; /* NB: no "volatile" */
#define ATOMIC_INIT(i)	{ (i) }
#define atomic_read(v)		((v)->counter)
static __inline__ int atomic_add_return(int i, atomic_t *v)
{
	int __i = i;
	__asm__ __volatile__(
		"lock; xaddl %0, %1;"
		:"=r"(i)
		:"m"(v->counter), "0"(i));
/*	__asm__ __volatile__(
		"lock; xaddl %0, %1"
		:"+r" (i), "+m" (v->counter)
		: : "memory"); */
	return i + __i;
}
int main (void) {
	atomic_t a = ATOMIC_INIT(0);
	int x;
	x = atomic_add_return (1, &a);
	if ((x!=1) || (atomic_read(&a)!=1))
		printf("fail: %i, %i\n", x, atomic_read(&a));
}