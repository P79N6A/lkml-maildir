Date: Tue, 31 Oct 2000 03:08:38 +0100
From: Andrea Arcangeli <>
Subject: Re: [PATCH] kiobuf/rawio fixes for 2.4.0-test10-pre6
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2000/10/30/160

On Mon, Oct 30, 2000 at 12:45:13PM +0100, Christoph Hellwig wrote:
> @@ -393,10 +396,15 @@
>  	pmd = pmd_offset(pgd, address);
>  	if (pmd) {
>  		pte_t * pte = pte_offset(pmd, address);
> -		if (pte && pte_present(*pte))
> +		if (pte && pte_present(*pte)) {
> +			if (writeacc && !pte_write(*pte))
> +				goto retry;
>  			return pte_page(*pte);
> +		}
>  	}
It should also make sure the pte is dirty before starting the read-from-disk
I/O and then things will currently break in the swapout because the page is not
locked (see discussion of last week). The fix for that problem proposed by SCT
and Linus is that the page (not pte) will be marked dirty during swapout and
written back to disk _only_ once reference count is 1 (btw I now noticed
invalidate_inode_pages+MAP_SHARED will mess with that fix and it will trigger a
BUG() in free_pages).
> +
> +faultin:
>  		if (handle_mm_fault(current->mm, vma, ptr, datain) <= 0) 
>  			goto out_unlock;
>  		spin_lock(&mm->page_table_lock);
> -		map = follow_page(ptr);
> -		if (!map) {
> +		map = follow_page(ptr, datain, &failed);
> +		if (failed) {
> +			/*
> +			 * Page got stolen before we could lock it down.
> +			 * Retry.
> +			 */
>  			spin_unlock(&mm->page_table_lock);
> -			dprintk (KERN_ERR "Missing page in map_user_kiobuf\n");
> -			goto out_unlock;
> +			goto faultin;
This is suboptimal (walks the pagetables twice if the page is just mapped). It
should be a follow page first and handle_mm_fault only if follow page failed.
Andrea
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
Please read the FAQ at 
http://www.tux.org/lkml/