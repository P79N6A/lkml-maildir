Date: Thu, 23 Mar 2000 18:49:06 -0600
From: Jesse Pollard <>
Subject: Re: Overcommitable memory??
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2000/3/23/161

On Wed, 22 Mar 2000, Matija Nalis wrote:
>On 21 Mar 2000 11:29:58 +0100, Jesse Pollard <pollard@cats-chateau.net> wrote:
>>On Sun, 19 Mar 2000, David Whysong wrote:
>>>Repeat after me:
>>>	You can not solve the OOM problem in user space.
>>>	You can not solve the OOM problem without killing processes.
>>>	Resource limits are not a good solution to the OOM problem.
>>>
>>>I don't like resource limits. Using resource limits is similar to not
>>>having memory overcommit -- you waste a lot of system resources "just in
>>>case", the kernel needs to do a lot more accounting, and it's just
>>>horribly inefficient.
>>
>>Resource limits CAN prevent the OOM condition if
>>	1. the sum of all concurrent users is <= total resources
>>	2. users are not allowed to exceed their quota
>
>No, they can't. Users are just one of the OOM causes. Imagine that no users
>allocate memory at this time (only have what they have allocated before).
>and then say syslog(8) wants to allocate memory, but there is no more. Or
>its stack just happens to grow and needs another page to allocate. Who do
>you kill ? syslog, which tried to use memory when there is no more (actually
>there is, but because of your no-overcommit policy it may not access it) ?
>Sysadmin, which did not anticipate that spare 100MB (after all users are
>withing their full limits) of memory would not be enough when some rare
>condition cases all system daemons to ask for some more memory ? Or do you
>also put VM quotas on root ? (which will kill some really old process, like
>init. Or sshd so sysadmin cannot login to fix problem)
Root has quotas too. The indications you are showing mean that the system
doesn't have enough quota. Quotas can be used, the system use must be measured.
>Or, as another example, quite a few TCP packets come in when system was low
>on memory (but all users maxing their per-user VM quotas) and system needs
>to allocate more network buffers. Who do you kill ? kernel, since it tried
>to allocate more memory ?
Actually - the same thing that happens now - TCP packets get dropped.
>Or VM tries to allocate disk buffers ? there are quite a few reasons for
>system to hit OOM even where there are ZERO user processes running at all,
>much less using too much memory.
Disk buffers are the first thing that are reduced when starting a new process.
>When that happens (and it will happen), you can kill some root owned process
>and try to continue, or you can panic. In first case, MAYBE system is as
>good as dead (if you killed process critical for system operation). In
>second, system is CERTAINLY dead. I prefer first case.
sure is - The system needs to be measured to determine how much is needed.
This is the same tuning cycle done as for users.
>When it hits OOM, kernel will start killing things. What I would prefer (and
>what I would code, if I needed it, which I don't at this time) is syscall
>which enables sysadmin to define order by which processes will be shot when
>time comes for this (see my previous post)
Unfortunately the list of processes that can be shot may not be valid at the
time it is needed.
System tuning is a necessity on any medium to large system. Tuning is
even necessary (even if usually done "by the seat of the pants") for
small desktop systems.
What is the purpose of the system?
What levels of service are needed?
How many users are going to be serviced?
How many organizations are going to be serviced?
What kind of maintenance schedule is needed?
How much memory do you need?
How much swap space do you want?
How fast a CPU (and how many) do you need?
How much disk storage do you need?
How much are you willing to spend?
All of this comes out of some form of requirements analysis. When the
answers are unknown, or sufficiently vague as to be useless, then it
get into measuring. Out of this comes the initial estimates for overhead.
Rules of thumb get into this too, especially for initial estimates.
For desktops used for word processing are used, the initial values are
taken from systems that are similar or from vendor recommendation. Thus
many desktop systems get 32-64MB ram, and some amount of swap. After all:
What is the purpose of the system?
	Desktop publishing - which vendor... most call for 32 to 128MB
What levels of service are needed?
	It needs to work during office hours.
How many users are going to be serviced?
	usually one at a time which translates into one.
How many organizations?
	usually one.
What kind of maintenance schedule is needed?
	Replace/fix it within 2 hours, if a spare is not available.
	Fix after hours if a spare is available.
How much memory do you need?
	Use the document generation software vendors recommendation.
How much swap space?
	Lets just guess that 64MB of disk will keep the system from ever
	overloading.
How fast a CPU...
	Usually one is enough. For most desktop publishing a 200MHz Pentium
	is overkill. If Image layout is part of the software, then the
	vendor will suggest 400MHz or higher (and suggest even more memory).
How much disk?
	For most things a 4GB disk is plenty. If the desktop is also
	going to end up being a 5 year history of all business transactions
	(email, contracts, etc.) then it may expand into 30GB. At that point
	the "purpose of the system" is no longer desktop publishing...
	go back to step 1, keep all the information, and try again
.....
The cycle continues. Large systems take considerably more analysis to determine
the answers. One of the problems I've been running into is that the amount
of memory neede by system processes isn't documented anywhere. It usually
boils doen to "If it will run Win xxx then it will run Linux better...".
This is acceptable for single user desktops, and even most software
development. It doesn't work when the applications (more than one) are estimated
to need 10-20GB of memory (virtual or not), or larger.
This (to me) appears to be the next hurdle than linux will leap... resource
control and advanced security features.
-------------------------------------------------------------------------
Jesse I Pollard, II
Email: pollard@cats-chateau.net
Any opinions expressed are solely my own.
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.rutgers.edu
Please read the FAQ at 
http://www.tux.org/lkml/