Date: Tue, 03 Feb 2004 16:53:59 +0900 (JST)
From:  j-nomura@ce ...
Subject: Re: [2.4] heavy-load under swap space shortage
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2004/2/3/12

Thanks for for your comment.
> Your patch just disables freeing mapped pages under memory pressure.
Right.
> You could try the untested patch below to swap_out_vma(), but I don't
> really recommend it: it still skips freeing up a less common category
> of clean pages, just when you'd most like to free them.
Hmm, your patch to swap_out_vma didn't solve the problem.
The main cause of the heavy load seems hard contention on page_table_lock.
The CPUs are virtually seriarized for swap_out_mm with each unfruitful
scannings.
The contention could be avoided by changing the spinlock to trylock.
How about the patch below?
With this change, the scannings become more efficient because they can be
done in parallel.
I'm not sure in this case whether the test for nr_swap_pages is necessary.
Best regards.
--
NOMURA, Jun'ichi <j-nomura@ce.jp.nec.com>
--- linux-2.4.24/mm/vmscan.c
+++ linux/mm/vmscan.c
@@ -292,7 +292,11 @@ static inline int swap_out_mm(struct mm_
 	 * Find the proper vm-area after freezing the vma chain 
 	 * and ptes.
 	 */
-	spin_lock(&mm->page_table_lock);
+	if (nr_swap_pages <= 0) {
+		if (!spin_trylock(&mm->page_table_lock))
+			return count; /* avoid contention */
+	} else
+		spin_lock(&mm->page_table_lock);
 	address = mm->swap_address;
 	if (address == TASK_SIZE || swap_mm != mm) {
 		/* We raced: don't count this mm but try again */
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  
http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  
http://www.tux.org/lkml/