Date: Fri, 8 Sep 2000 16:58:33 +0400
From: Ivan Kokshaysky <>
Subject: Re: Compilation failure on Alpha with test8-pre[2-6]
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2000/9/8/37

On Fri, Sep 08, 2000 at 04:19:25AM -0400, Christopher C. Chimelis wrote:
> xor.c: In function `xor_block_alpha':
> xor.c:1791: inconsistent operand constraints in an `asm'
> xor.c: In function `xor_block_alpha_prefetch':
> xor.c:2213: inconsistent operand constraints in an `asm'
> 
Yes, I can reproduce this with gcc-2.95.2 (compiles cleanly with 2.96).
Looks like older gcc doesn't like when output operand 5 listed
also as input. Hmm.
Simple swapping operands 4 and 5 makes gcc happy.
Ivan.
--- 2.4.0p8t5/drivers/block/xor.c	Wed Sep  6 11:57:49 2000
+++ linux/drivers/block/xor.c	Fri Sep  8 16:20:27 2000
@@ -1667,12 +1667,12 @@
 	ldq %1,8(%6)
 	ldq %2,16(%6)
 	ldq %3,24(%6)
-	ldq %4,32(%6)
+	ldq %5,32(%6)
 	ldq %0,%7(%0)
 	ldq %1,%7(%1)
 	ldq %2,%7(%2)
 	ldq %3,%7(%3)
-	ldq %4,%7(%4)
+	ldq %5,%7(%5)
 	.align 4
 5:
 	ldq $0,0(%0)
@@ -1680,13 +1680,13 @@
 	ldq $2,0(%2)
 	ldq $3,0(%3)
 
-	ldq $4,0(%4)
+	ldq $4,0(%5)
 	ldq $5,8(%0)
 	ldq $6,8(%1)
 	ldq $7,8(%2)
 
 	ldq $16,8(%3)
-	ldq $17,8(%4)
+	ldq $17,8(%5)
 	ldq $18,16(%0)
 	ldq $19,16(%1)
 
@@ -1695,7 +1695,7 @@
 	ldq $21,16(%3)
 	xor $2,$3,$3		# 6 cycles from $3 load
 
-	ldq $0,16(%4)
+	ldq $0,16(%5)
 	xor $1,$3,$3
 	ldq $1,24(%0)
 	xor $3,$4,$4		# 7 cycles from $4 load
@@ -1715,7 +1715,7 @@
 	ldq $4,24(%3)
 	xor $21,$0,$0		# 7 cycles from $0 load
 
-	ldq $5,24(%4)
+	ldq $5,24(%5)
 	xor $20,$0,$0
 	ldq $6,32(%0)
 	ldq $7,32(%1)
@@ -1727,7 +1727,7 @@
 	
 	ldq $17,32(%3)
 	xor $2,$4,$4
-	ldq $18,32(%4)
+	ldq $18,32(%5)
 	ldq $19,40(%0)
 
 	ldq $20,40(%1)
@@ -1737,7 +1737,7 @@
 
 	stq $5,24(%0)
 	xor $6,$7,$7		# 7 cycles from $7 load
-	ldq $1,40(%4)
+	ldq $1,40(%5)
 	ldq $2,48(%0)
 
 	ldq $3,48(%1)
@@ -1747,7 +1747,7 @@
 
 	ldq $5,48(%3)
 	xor $16,$18,$18
-	ldq $6,48(%4)
+	ldq $6,48(%5)
 	xor $19,$20,$20		# 7 cycles from $20 load
 
 	stq $18,32(%0)
@@ -1758,7 +1758,7 @@
 	ldq $16,56(%1)
 	ldq $17,56(%2)
 	ldq $18,56(%3)
-	ldq $19,56(%4)
+	ldq $19,56(%5)
 
 	xor $21,$1,$1
 	xor $2,$3,$3		# 9 cycles from $3 load
@@ -1772,21 +1772,21 @@
 
 	stq $6,48(%0)
 	xor $16,$18,$18
-	subq %5,1,%5
+	subq %4,1,%4
 	xor $18,$19,$19		# 8 cycles from $19 load
 
 	stq $19,56(%0)
-	addq %4,64,%4
+	addq %5,64,%5
 	addq %3,64,%3
 	addq %2,64,%2
 
 	addq %1,64,%1
 	addq %0,64,%0
-	bgt %5,5b"
-	: "=&r"(d), "=&r"(s1), "=&r"(s2), "=&r"(s3), "=r"(s4), "=r"(lines)
+	bgt %4,5b"
+	: "=&r"(d), "=&r"(s1), "=&r"(s2), "=&r"(s3), "=r"(lines), "=r"(s4)
 	/* ARG! We've run out of asm arguments!  We've got to reload
 	   all those pointers we just loaded.  */
-	: "r"(bh_ptr), "i" (&((struct buffer_head *)0)->b_data), "5"(lines)
+	: "r"(bh_ptr), "i" (&((struct buffer_head *)0)->b_data), "4"(lines)
 	: "memory", "$0", "$1", "$2", "$3", "$4", "$5", "$6", "$7",
 	  "$16", "$17", "$18", "$19", "$20", "$21");
 	return;
@@ -2084,12 +2084,12 @@
 	ldq %1,8(%6)
 	ldq %2,16(%6)
 	ldq %3,24(%6)
-	ldq %4,32(%6)
+	ldq %5,32(%6)
 	ldq %0,%7(%0)
 	ldq %1,%7(%1)
 	ldq %2,%7(%2)
 	ldq %3,%7(%3)
-	ldq %4,%7(%4)
+	ldq %5,%7(%5)
 	.align 4
 5:
 	ldq $0,0(%0)
@@ -2097,13 +2097,13 @@
 	ldq $2,0(%2)
 	ldq $3,0(%3)
 
-	ldq $4,0(%4)
+	ldq $4,0(%5)
 	ldq $5,8(%0)
 	ldq $6,8(%1)
 	ldq $7,8(%2)
 
 	ldq $16,8(%3)
-	ldq $17,8(%4)
+	ldq $17,8(%5)
 	ldq $18,16(%0)
 	ldq $19,16(%1)
 
@@ -2112,7 +2112,7 @@
 	ldq $21,16(%3)
 	xor $2,$3,$3		# 6 cycles from $3 load
 
-	ldq $0,16(%4)
+	ldq $0,16(%5)
 	xor $1,$3,$3
 	ldq $1,24(%0)
 	xor $3,$4,$4		# 7 cycles from $4 load
@@ -2132,7 +2132,7 @@
 	ldq $4,24(%3)
 	xor $21,$0,$0		# 7 cycles from $0 load
 
-	ldq $5,24(%4)
+	ldq $5,24(%5)
 	xor $20,$0,$0
 	ldq $6,32(%0)
 	ldq $7,32(%1)
@@ -2144,7 +2144,7 @@
 	
 	ldq $17,32(%3)
 	xor $2,$4,$4
-	ldq $18,32(%4)
+	ldq $18,32(%5)
 	ldq $19,40(%0)
 
 	ldq $20,40(%1)
@@ -2154,7 +2154,7 @@
 
 	stq $5,24(%0)
 	xor $6,$7,$7		# 7 cycles from $7 load
-	ldq $1,40(%4)
+	ldq $1,40(%5)
 	ldq $2,48(%0)
 
 	ldq $3,48(%1)
@@ -2164,7 +2164,7 @@
 
 	ldq $5,48(%3)
 	xor $16,$18,$18
-	ldq $6,48(%4)
+	ldq $6,48(%5)
 	xor $19,$20,$20		# 7 cycles from $20 load
 
 	stq $18,32(%0)
@@ -2175,7 +2175,7 @@
 	ldq $16,56(%1)
 	ldq $17,56(%2)
 	ldq $18,56(%3)
-	ldq $19,56(%4)
+	ldq $19,56(%5)
 
 	ldq $31,256(%0)
 	xor $21,$1,$1
@@ -2187,28 +2187,28 @@
 	ldq $31,256(%3)
 	xor $5,$6,$6		# 8 cycles from $6 load
 
-	ldq $31,256(%4)
+	ldq $31,256(%5)
 	xor $4,$6,$6
 	xor $7,$16,$16		# 7 cycles from $16 load
 	xor $17,$18,$18		# 6 cycles from $18 load
 
 	stq $6,48(%0)
 	xor $16,$18,$18
-	subq %5,1,%5
+	subq %4,1,%4
 	xor $18,$19,$19		# 8 cycles from $19 load
 
 	stq $19,56(%0)
-	addq %4,64,%4
+	addq %5,64,%5
 	addq %3,64,%3
 	addq %2,64,%2
 
 	addq %1,64,%1
 	addq %0,64,%0
-	bgt %5,5b"
-	: "=&r"(d), "=&r"(s1), "=&r"(s2), "=&r"(s3), "=r"(s4), "=r"(lines)
+	bgt %4,5b"
+	: "=&r"(d), "=&r"(s1), "=&r"(s2), "=&r"(s3), "=r"(lines), "=r"(s4)
 	/* ARG! We've run out of asm arguments!  We've got to reload
 	   all those pointers we just loaded.  */
-	: "r"(bh_ptr), "i" (&((struct buffer_head *)0)->b_data), "5"(lines)
+	: "r"(bh_ptr), "i" (&((struct buffer_head *)0)->b_data), "4"(lines)
 	: "memory", "$0", "$1", "$2", "$3", "$4", "$5", "$6", "$7",
 	  "$16", "$17", "$18", "$19", "$20", "$21");
 	return;
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
Please read the FAQ at 
http://www.tux.org/lkml/