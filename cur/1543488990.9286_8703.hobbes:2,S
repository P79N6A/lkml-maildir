Date: Mon, 8 Mar 1999 14:01:37 +1100
From: Andrew Tridgell <>
Subject: Re: Lets get this right (WAS RE:MOSIX and kernel mods)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/1999/3/7/150

There are real applications where DIPC makes sense, but they generally
won't be the sorts of applications that are developed in HPC labs.
Here are some real examples of where DIPC would make sense:
- a neural network training program. As part of my PhD studies I wrote
  a distributed recurrent neural network training program. It
  typically used dozens or even hundreds of hours of cpu time on a 32
  node CM5 (a distributed memory sparc based machine with two vector
  units on each node) to train the network for recognising speech
  data. Training a neural network is an example of an "embarrassingly
  parallel" task, it needs very little comunications and when it does
  communicate you are basically updating a small shared structure. I
  coded this using a message passing library but it would have actually
  been simpler to use a software DSM system had it been available as
  the performance of the communications was non-critical.
- if you wanted to port Samba to a cluster then it would make sense to
  at least prototype the port using something like DIPC. Samba uses
  shared memory (either mmap or sysv) for "share modes" and oplock
  structures but these (mostly) only need to be accessed at a file
  open/close operations. It won't be optimal to use a DSM system for a
  cluster port but it would get it going quickly and you could then
  look at what the bottlenecks are and decide on a more optimised
  approach. 
- my parallel chess program (KnightCap) could use DIPC quite
  effectively. You would initially use it for everything then would
  slowly replace the time critical parts (basically the distributed
  hash tables) with message passing. My high-performance port of
  KnightCap to the AP+ used the native aplib message passing library
  (faster and simpler than MPI) but that relies on hardware
  assistance. For a fast-ethernet Linux cluster DIPC for the top level
  position structures (only accessed every few seconds) and UDP
  messages for hash table updates (it doesn't matter if you lose a few
  of those) would make sense.
I'm sure there are other examples.
I'd also point out that MPI isn't quite the ideal solution that some
posts (Hi Larry!) have portrayed it as. In MPI-1 you don't have any
async operations, so you can't say "send this data to that address on
that node" (a "put" operation). The result of that lack is that you
have to either poll or have a little helper process sitting around
waiting for data (or use a worker-farm model). That can be very
inconvenient, especially if comms isn't the central part of your
program. The fact that MPI-1 isn't thread-safe doesn't help. You also
can't use select() or poll() to wait for MPI messages so you really
have to abandon the normal unix wait-for-a-event system and move your
whole program to message passing. That can be a big leap.
In MPI-2 you do have put/get type operations (essentially remote
memory copy) but you still can't use poll() or select(). You can have
a async message handler though, so you don't necessarily need a helper
process. Our local version of MPI-2 has an extension that can give you
a file descriptor to wait for messages on.
I'm not saying that systems like DIPC are the solution to the worlds
cluster problems. They aren't. For communicatons intensive programs
the performance will most likely be lousy. They are useful in some
circumstances though and I don't think they should be dismissed
completely. Especially if you combine them with a better (ie. lower
latency) transport than TCP/IP.
Cheers, Tridge
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.rutgers.edu
Please read the FAQ at 
http://www.tux.org/lkml/