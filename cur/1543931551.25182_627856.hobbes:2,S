Date: Thu, 13 Dec 2007 14:52:32 -0800 (PST)
From: Christoph Lameter <>
Subject: Re: What was the problem with quicklists and x86-64?
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2007/12/13/499

On Thu, 13 Dec 2007, Siddha, Suresh B wrote:
> <snip>
> Oh, and I see what's wrong: you not only switched "free_page()" to 
> "quicklist_free()", you *also* switched "tlb_remove_page()" to 
> "quicklist_free()".
> </snip>
> 
> The above comment is in reference to below portion of code:
> 
> -#define __pte_free_tlb(tlb,pte) tlb_remove_page((tlb),(pte))
> +#define __pte_free_tlb(tlb,pte) quicklist_free_page(QUICK_PT, NULL,(pte))
> 
> tlb_remove_page() was marking tlb->need_flush. Which is later used
> by tlb_flush_mmu(). With quicklist_free_page() we loose all that..
Aha. So we need to flush if there is a page on the quicklist. Patch 
follows.
> Now in a corner case scenario  with a big munmap() which calls unmap_region()
> and if it so happens that the region getting unmapped just has page
> tables setup but with all PTE's set to NULL, unmap_region() may
> potentially free the page table pages
> 	[ tlb_finish_mmu() calls check_pgt_cache() which trims quicklist ]
> with out flushing the TLB's.
> 	[ (tlb_finish_mmu() calls the tlb_flush_mmu() but it will not do
>            much as need_flush is not set ]
> 
> Similarly Linus brought pre-emptions issues associated with quicklist usage..
AFAICT page table shootdown cannot be preempted. What where the preemption issues?
Index: linux-2.6/include/asm-generic/tlb.h
===================================================================
--- linux-2.6.orig/include/asm-generic/tlb.h	2007-12-13 14:45:38.000000000 -0800
+++ linux-2.6/include/asm-generic/tlb.h	2007-12-13 14:51:07.000000000 -0800
@@ -14,6 +14,7 @@
 #define _ASM_GENERIC__TLB_H
 
 #include <linux/swap.h>
+#include <linux/quicklist.h>
 #include <asm/pgalloc.h>
 #include <asm/tlbflush.h>
 
@@ -85,6 +86,9 @@ tlb_flush_mmu(struct mmu_gather *tlb, un
 static inline void
 tlb_finish_mmu(struct mmu_gather *tlb, unsigned long start, unsigned long end)
 {
+#ifdef CONFIG_QUICKLIST
+	tlb->need_flush += &__get_cpu_var(quicklist)[0].nr_pages != 0;
+#endif
 	tlb_flush_mmu(tlb, start, end);
 
 	/* keep the page table cache within bounds */