Date: Wed, 9 Jan 2008 22:57:25 +0100
From: Andi Kleen <>
Subject: STT_FUNC for assembler checksum and semaphore ops" in git-x86
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2008/1/9/297

In gitx86:
commit 692effca950d7c6032e8e2ae785a32383e7af4a3
Author: John Reiser <jreiser@BitWagon.com>
Date:   Wed Jan 9 13:31:12 2008 +0100
    STT_FUNC for assembler checksum and semaphore ops
...
    Comments?
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
diff --git a/arch/x86/lib/checksum_32.S b/arch/x86/lib/checksum_32.S
index adbccd0..1f9aacb 100644
--- a/arch/x86/lib/checksum_32.S
+++ b/arch/x86/lib/checksum_32.S
@@ -48,6 +48,7 @@ unsigned int csum_partial(const unsigned char * buff, int len, unsigned int sum)
           * Fortunately, it is easy to convert 2-byte alignment to 4-byte
           * alignment for the unrolled loop.
           */           
+       .type csum_partial, @function
 ENTRY(csum_partial)
+       .type csum_partial, @function
 ENTRY(csum_partial)
        CFI_STARTPROC
        pushl %esi
@@ -141,11 +142,13 @@ ENTRY(csum_partial)
        ret
        CFI_ENDPROC
 ENDPROC(csum_partial)
+       .size csum_partial, . - csum_partial
AK:
Better option would be to just add to ENTRY/ENDPROC
do something like (untested) 
#define ENTRY(x) \
	...
	.set curfunc, x
#define ENDPROC(x) \
	...
	.size x - curfunc
-Andi