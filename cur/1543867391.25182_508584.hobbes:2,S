Date: Wed, 14 Feb 2007 12:13:48 +0300
From: Evgeniy Polyakov <>
Subject: Re: [patch 05/11] syslets: core code
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2007/2/14/45

On Tue, Feb 13, 2007 at 11:41:31PM +0100, Ingo Molnar (mingo@elte.hu) wrote:
> > Then limit it to a single page and use gup
> 
> 1024 (512 on 64-bit) is alot but not ALOT. It is also certainly not 
> ALOOOOT :-) Really, people will want to have more than 512 
> disks/spindles in the same box. I have used such a beast myself. For Tux 
> workloads and benchmarks we had parallelism levels of millions of 
> pending requests (!) on a single system - networking, socket limits, 
> disk IO combined with thousands of clients do create such scenarios. I 
> really think that such 'pinned pages' are a pretty natural fit for 
> sys_mlock() and RLIMIT_MEMLOCK, and since the kernel side is careful to 
> use the _inatomic() uaccess methods, it's safe (and fast) as well.
This will end up badly - I used the same approach in the early kevent
days and was proven to have swapable memory for the ring. I think it
would be much better to have userspace allocated ring and use
copy_to_user() there.
Btw, as a bit of advertisement, the whole completion part can be done
through kevent which already has ring buffer, queue operations and
non-racy updates... :)
> 	Ingo
-- 
	Evgeniy Polyakov
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  
http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  
http://www.tux.org/lkml/