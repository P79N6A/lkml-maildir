Date: Wed, 7 May 2008 19:14:33 -0400
From: "Morten Welinder" <>
Subject: Re: Deleting large files
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2008/5/7/394

>  Suppose you had an N GB file that just filled up the disk. You now
>  delete it, but get control back before it is really deleted. You
>  now start to write a new file that will eventually just fill up
>  the disk. [...]
That argument ought to stop right there.  If you believe that deleting a
file will necessarily and immediately give you back the space, then you
are wrong in the current state of the affairs already.
NFS does not do that -- in fact, I don't believe any file system does that
unless you can guarantee at least that no other process or the kernel has
that file open; AFS did not do that last I looked a decade ago; versioning
file systems do not; journaling file systems might not.  File systems that
support undelete do not do that.  In short: assuming such a thing is a
bug in need of a fix today.
Right now, unlink is a commonly used syscall with unbounded response
time.  If your GUI program deletes a file, the GUI generally locks up until
the kernel feels like returning -- that is certainly not how you get a smooth
user experience.  Forking a process to do the deletion (a) is pathetic,
(b) is not currently done, and (c) does not work: you cannot get a result
right away, i.e., you lose error handling.
Morten