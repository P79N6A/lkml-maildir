Date: Sun, 28 Nov 2004 13:16:17 +0300
From: Oleg Nesterov <>
Subject: Re: PATCH? rcu: eliminate rcu_ctrlblk.lock
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2004/11/28/11

Manfred Spraul wrote:
>
> Your patch would add one new corner case:
>
> start: next_pending==1. rcp->cur == 11.
> cpu 1: rcu_start_back sets next_pending to 0.
> cpu 2: rdp->batch = rcp->cur + 1 [i.e. wait for end of period 12]
> cpu 2: notices next_pending == 0, tries to acquire the spinlock [blocks]
> cpu 1: rcp->cur++ [i.e. start period 12]
> cpu 1: releases the spinlock
> cpu 2: gets the spinlock, sets next_pending to 1 and exits.
>
> Now next_pending is 1 [i.e. at the end of grace period 12 grace period
> 13 is automatically started], although noone has callbacks waiting for
> period 13.
Yes. But if i understand correctly, the current behaviour a bit worse.
In this scenario rcu_process_callbacks() on cpu 2 will re-read cur==12
and next_pendind==0 and call start_batch(), so grace period 13 will be
started at the end of grace period 12 anyway.
The difference is that with this patch the 'curlist' will be flushed when
the grace period 12 is completed, while the current code will postpone it
up to 13.
Oleg.
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  
http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  
http://www.tux.org/lkml/