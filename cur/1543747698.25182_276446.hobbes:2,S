Date: Fri, 03 Dec 2004 11:45:03 +0100
From: "Prakash K. Cheemplavam" <>
Subject: Re: Time sliced CFQ io scheduler
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2004/12/3/47

Jens Axboe schrieb:
> On Fri, Dec 03 2004, Jens Axboe wrote:
> 
>>Funky. It looks like another case of the io scheduler being at the wrong
>>place - if raid sends dependent reads to different drives, it screws up
>>the io scheduling. The right way to fix that would be to io scheduler
>>before raid (reverse of what we do now), but that is a lot of work. A
>>hack would be to try and tie processes to one md component for periods
>>of time, sort of like cfq slicing.
> 
> 
> It makes sense to split the slice period for sync and async requests,
> since async requests usually get a lot of requests queued in a short
> period of time. Might even make sense to introduce a slice_rq value as
> well, limiting the number of requests queued in a given slice.
> 
> But at least this patch lets you set slice_sync and slice_async
> seperately, if you want to experiement.
An idea, which values I should try?
In generell I rather have the impression the problem I am experiencing 
is not the problem of the io scheduler alone or why do all show the same 
problem?
BTW, I just did my little test on the ide drive and it shows the same 
problem, so it is not sata / libata related.
Prakash
[unhandled content-type:application/pgp-signature]