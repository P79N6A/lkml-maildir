Date: Thu, 01 Jan 2009 08:46:03 +0100
From: Mike Galbraith <>
Subject: [patch] Re: problem with "sched: revert back to per-rq vruntime"?
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2009/1/1/5

Would perhaps be prettier to have the load already in place at call
time, but methinks the enqueue/dequeue accounting logic is nice as is,
so complete the unlikely case handling in an unlikely block.
Impact: bug fixlet.
Account for tasks which have not yet been enqueued in calc_delta_weight().
Signed-off-by: Mike Galbraith <efault@gmx.de>
diff --git a/kernel/sched_fair.c b/kernel/sched_fair.c
index 5ad4440..4685f28 100644
--- a/kernel/sched_fair.c
+++ b/kernel/sched_fair.c
@@ -392,8 +392,16 @@ static inline unsigned long
 calc_delta_weight(unsigned long delta, struct sched_entity *se)
 {
 	for_each_sched_entity(se) {
-		delta = calc_delta_mine(delta,
-				se->load.weight, &cfs_rq_of(se)->load);
+		struct load_weight *load = &cfs_rq_of(se)->load;
+
+		if (unlikely(!se->on_rq)) {
+			struct load_weight tmp;
+
+			tmp.weight = load->weight + se->load.weight;
+			tmp.inv_weight = 0;
+			load = &tmp;
+		}
+		delta = calc_delta_mine(delta, se->load.weight, load);
 	}
 
 	return delta;