Date: Wed, 14 Apr 1999 18:57:02 -0700 (PDT)
From: Alex Belits <>
Subject: Re: NT reported faster than Linux 2.2.2 SMP
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/1999/4/15/45

On Wed, 14 Apr 1999, Robert G. 'Doc' Savage wrote:
> Mindcraft also used the v0.92 MegaRAID driver.  An SMP race condition was
> fixed in v0.93 which was almost certainly available from the AMI web site
> long before the Mar 10-13 test.  So SMP NT "beat" a non-SMP Linux on a
> quad-Xeon server.  Big hairy deal.
  Did race condition cause SMP kernel to fail in their tests, so they used
UP kernel, or it just crippled the performance? Their report is unclear on
that part, listing Linux configuration as SMP, but telling that SMP didn't
work with RAID.
> After COMDEX I'm upgrading my dual-Pentium/233 system with a MegaRAID 428
> using Linux 2.2.5-ac6 (or later) and the Apache Secure Server from RH5.2.
> I won't have 144 client PCs to duplicate Mindcraft's test load, but I do
> have a P-II/266 laptop and 100 Mbps connectivity to the big system.  Can
> anyone suggest a way to simulate high loads from one client?
  They tested configuration with four ethernet cards to use transfer rates
above 100Mbps. While more or less reasonable for a huge filerserver, I
consider such thing on a web server unrealistic -- one _can_ have this
kind of load on a web server, but network latency will be much higher than
that their network had, and that affects HTTP performance significantly,
and in different ways on different server types. To have low latency one
needs to have Ethernet (or better) and no routers between client and
server, but I can't imagine switched Ethernet network large enough to
create this kind of load on a web server in any realistic usage, so while
it may be a nice test, the results from it will have only academic value.
  OTOH, web servers *are* built on large boxes, but not because they have
400Mbps to saturate but because they have heavy backend processing --
applications/scripting/databases. It will be nice to see, how the
performance of different OS differs on those. And not braindead cases,
like CGI, or, worse, CGI on everything vs. ISAPI on NT (like CGI is the
only thing everywhere else), but, say, ASP with perl (or VB) vs. mod_perl
(or PHP).
  Currently there is no usable benchmark for HTTP servers that can be
considered to test servers under realistic conditions, and this situation
confuses the hell out of people. I tried to do some simple testing of my
own HTTP server, and found that things like ab/zb produced ridiculous 
"load pattern" (if the same client repeatedly requesting the same URL can 
be called a pattern) that favored all kinds of caching, long-living
processes or threads, etc.
-- 
Alex
----------------------------------------------------------------------
 Excellent.. now give users the option to cut your hair you hippie!
                                                  -- Anonymous Coward
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.rutgers.edu
Please read the FAQ at 
http://www.tux.org/lkml/