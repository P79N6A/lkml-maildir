Date: Wed, 16 Jan 2008 21:50:04 -0500
From: Mathieu Desnoyers <>
Subject: Re: [RFC PATCH 16/22 -v2] add get_monotonic_cycles
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2008/1/16/471

* Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca) wrote:
> * john stultz (johnstul@us.ibm.com) wrote:
> > 
> > On Wed, 2008-01-16 at 18:33 -0500, Steven Rostedt wrote:
> > > Thanks John for doing this!
> > > 
> > > (comments imbedded)
> > > 
> > > On Wed, 16 Jan 2008, john stultz wrote:
> > > > +	int num = !cs->base_num;
> > > > +	cycle_t offset = (now - cs->base[!num].cycle_base_last);
> > > > +	offset &= cs->mask;
> > > > +	cs->base[num].cycle_base = cs->base[!num].cycle_base + offset;
> > > > +	cs->base[num].cycle_base_last = now;
> > > 
> > > I would think that we would need some sort of barrier here. Otherwise,
> > > base_num could be updated before all the cycle_base. I'd expect a smp_wmb
> > > is needed.
> > 
> > Hopefully addressed in the current version.
> > 
> > 
> > > > Index: monotonic-cleanup/kernel/time/timekeeping.c
> > > > ===================================================================
> > > > --- monotonic-cleanup.orig/kernel/time/timekeeping.c	2008-01-16 12:21:46.000000000 -0800
> > > > +++ monotonic-cleanup/kernel/time/timekeeping.c	2008-01-16 14:15:31.000000000 -0800
> > > > @@ -71,10 +71,12 @@
> > > >   */
> > > >  static inline s64 __get_nsec_offset(void)
> > > >  {
> > > > -	cycle_t cycle_delta;
> > > > +	cycle_t now, cycle_delta;
> > > >  	s64 ns_offset;
> > > >
> > > > -	cycle_delta = clocksource_get_cycles(clock, clocksource_read(clock));
> > > > +	now = clocksource_read(clock);
> > > > +	cycle_delta = (now - clock->cycle_last) & clock->mask;
> > > > +	cycle_delta += clock->cycle_accumulated;
> > > 
> > > Is the above just to decouple the two methods?
> > 
> > Yep. clocksource_get_cycles() ended up not being as useful as an helper
> > function (I was hoping the arch vsyscall implementations could use it,
> > but they've done too much optimization - although that may reflect a
> > need up the chain to the clocksource structure).
> > 
> 
> The problem with vsyscall is that we will have a hard time disabling
> preemption :( Therefore, insuring that the read of the data is done in a
> timely manner is hard to do.
> 
Sorry for self-reply, but I thought, in the past, of a way to make this
possible.
It would imply the creation of a new vsyscall : vgetschedperiod
It would read a counter that would increment each time the thread is
scheduled out (or in). It would be a per thread counter (not a per cpu
counter) so we can deal appropriately with a stopped thread that would
happen to come back running a loooong time afterward (if we do per-cpu
counters, we could get the same 32 bits counter value falsely if it is
shared with other thread activity).
Then, the clocksource read code would look like :
int period;
do {
  period = vgetschedperiod();
  perform the clocksource read..
} while (period != vgetschedperiod());
Therefore, we would be sure that we have not been scheduled out while
reading the value. I think this new vsyscall could be useful for others.
Actually, it would make implementation of RCU in user-space possible (as
long as the read-side can retry the read operation).
Mathieu
> 
> > thanks
> > -john
> > 
> > 
> 
> -- 
> Mathieu Desnoyers
> Computer Engineering Ph.D. Student, Ecole Polytechnique de Montreal
> OpenPGP key fingerprint: 8CD5 52C3 8E3C 4140 715F  BA06 3F25 A8FE 3BAE 9A68
-- 
Mathieu Desnoyers
Computer Engineering Ph.D. Student, Ecole Polytechnique de Montreal
OpenPGP key fingerprint: 8CD5 52C3 8E3C 4140 715F  BA06 3F25 A8FE 3BAE 9A68