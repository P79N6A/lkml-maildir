Date: Mon, 31 Jul 2000 15:00:26 -0400
From:  marek@foundmon ...
Subject: Re: 2GIG-file
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2000/7/31/101

1) The file isn't fragmented on the the NTFS.
2) I really need to find a solution to this DB ported to Linux today.
Currently I am compiling 2.3.99pre9, any comments for running this as a
production kernel, I know its in dev, but what other choices do I have ?
"Jeff V. Merkey" wrote:
> Very good question.  I've wondered the same myself.  The current
> interface seems to assume 2GB since the sign bit is used (off_t is
> defined from __kernel_off_t which is typedef'd as a 'signed long').
> The base definition is in posix_types.h.  Linux needs to expand this to
> support filesizes up to the limit of the architeture (i.e. 4GB files).
>
> Not to be poking fun or anyhing, but if you have a 3.4 GB file on NT, it
> must be pretty fragmented, and even if you could copy it, NTFS may take
> a very long time with a file of this size.  My experience with files on
> NTFS this large, particularly databse files, if that they get super
> fragmented, and take hours or even days to copy.  I am guessing you are
> looking at Linux because you have a super fragmented monster file and
> are getting incredibly slow access times on NT right now?
>
> There are tools available from MS vendors for this fragmentation problem
> -- email davidg@balder.com and he can point you to some great defrag
> tools he wrote for MS and some of MS customers just for this huge file
> fragmentation problem.  Because of the way NTFS is designed, as random
> access files grow to huge sizes, fragmentation can bite hard on
> performance.  I think you may be in an area where Linux has not caught
> up to W2K and support these large files.
>
> Dear Al, Linus, and Alan -- 2GB liit is probably something to look at
> expanding in the future to support these huge SQL server database files.
>
> :-)
>
> Jeff
>
> marek@foundmoney.com wrote:
> >
> > I have a 3.4 Gig file(on NT) that I need to import into a database on
> > unix machine, RH6.2, knowing there's a limitation of 2 GIG how would you
> >
> > suggest I do this ?,
> >
> > Then can MYSQL hold that much data with the file limitation(does it
> > automatically split up files) ?
> >
> > Thank you
> >
> > -
> > To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
> > the body of a message to majordomo@vger.rutgers.edu
> > Please read the FAQ at 
http://www.tux.org/lkml/
>
> -
> To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
> the body of a message to majordomo@vger.rutgers.edu
> Please read the FAQ at 
http://www.tux.org/lkml/
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.rutgers.edu
Please read the FAQ at 
http://www.tux.org/lkml/