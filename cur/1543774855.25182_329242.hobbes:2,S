Date: Tue, 28 Jun 2005 10:43:29 +1000
From: Peter Williams <>
Subject: Re: [PATCH] sched: consider migration thread with smp nice
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2005/6/27/375

Con Kolivas wrote:
> This patch improves throughput with the smp nice balancing code. Many thanks 
> to Martin Bligh for the usage of his regression testing bed to confirm the 
> effectiveness of various patches.
Con,
	This doesn't build on non SMP systems due to the migration_thread field 
only being defined for SMP.  Attached is a copy of a slightly modified 
PlugSched version of the patch which I used to fix the problem in 
PlugSched.  Even though it's for a different file it should be easy to 
copy over.
Peter
-- 
Peter Williams                                   pwil3058@bigpond.net.au
"Learning, n. The kind of ignorance distinguishing the studious."
  -- Ambrose Bierce
Index: MM-2.6.12/include/linux/sched_pvt.h
===================================================================
--- MM-2.6.12.orig/include/linux/sched_pvt.h	2005-06-28 10:11:47.000000000 +1000
+++ MM-2.6.12/include/linux/sched_pvt.h	2005-06-28 10:37:14.000000000 +1000
@@ -393,6 +393,11 @@
 {
 	rq->prio_bias -= MAX_STATIC_PRIO - prio;
 }
+
+static inline int is_migration_thread(const task_t *p, const runqueue_t *rq)
+{
+	return p == rq->migration_thread;
+}
 #else
 static inline void inc_prio_bias(runqueue_t *rq, int prio)
 {
@@ -401,23 +406,35 @@
 static inline void dec_prio_bias(runqueue_t *rq, int prio)
 {
 }
+
+static inline int is_migration_thread(const task_t *p, const runqueue_t *rq)
+{
+	return 0;
+}
 #endif
 
 static inline void inc_nr_running(task_t *p, runqueue_t *rq)
 {
 	rq->nr_running++;
-	if (rt_task(p))
-		inc_prio_bias(rq, p->prio);
-	else
+	if (rt_task(p)) {
+		if (!is_migration_thread(p, rq))
+			/*
+			 * The migration thread does the actual balancing. Do
+			 * not bias by its priority as the ultra high priority
+			 * will skew balancing adversely.
+			 */
+			inc_prio_bias(rq, p->prio);
+	} else
 		inc_prio_bias(rq, p->static_prio);
 }
 
 static inline void dec_nr_running(task_t *p, runqueue_t *rq)
 {
 	rq->nr_running--;
-	if (rt_task(p))
-		dec_prio_bias(rq, p->prio);
-	else
+	if (rt_task(p)) {
+		if (!is_migration_thread(p, rq))
+			dec_prio_bias(rq, p->prio);
+	} else
 		dec_prio_bias(rq, p->static_prio);
 }
 