Date: Tue, 04 Apr 2006 16:19:17 +1000
From: Peter Williams <>
Subject: [PATCH] sched: improve smpnice load balancing when load per task imbalanced
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2006/4/4/19

Problem:
2 CPU system: if the cpu-0 has two high priority and cpu-1 has one 
normal priority task, how can the current code detect this imbalance 
because imbalance will be always < busiest_load_per_task and max_load - 
this_load will be < 2 * busiest_load_per_task and pwr_move will be <= 
pwr_now.
Solution:
Modify the assessment of small imbalances to take into account the 
relative sizes of busiest_load_per_task and this_load_per_task.  This is 
exploiting the fact that if the difference between the loads is greater 
than busiest_load_per_task and busiest_load_per_task is greater than 
this_load_per_task then moving busiest_load_per_task worth of load from 
busiest to this will be an improvement in the distribution of weighted load.
Required patches:
sched-prevent-high-load-weight-tasks-suppressing-balancing.patch
sched-improve-stability-of-smpnice-load-balancing.patch
Note: This patch makes no change to load balancing in the case where all 
tasks are nice==0.
Signed-off-by: Peter Williams <pwil3058@bigpond.com.au>
-- 
Peter Williams                                   pwil3058@bigpond.net.au
"Learning, n. The kind of ignorance distinguishing the studious."
   -- Ambrose Bierce
Index: MM-2.6.X/kernel/sched.c
===================================================================
--- MM-2.6.X.orig/kernel/sched.c	2006-04-04 15:18:19.000000000 +1000
+++ MM-2.6.X/kernel/sched.c	2006-04-04 15:53:48.000000000 +1000
@@ -2252,8 +2252,16 @@ find_busiest_group(struct sched_domain *
 	if (*imbalance < busiest_load_per_task) {
 		unsigned long pwr_now = 0, pwr_move = 0;
 		unsigned long tmp;
+		unsigned int imbn = 2;
 
-		if (max_load - this_load >= busiest_load_per_task*2) {
+		if (this_nr_running) {
+			this_load_per_task /= this_nr_running;
+			if (busiest_load_per_task > this_load_per_task)
+				imbn = 1;
+		} else
+			this_load_per_task = SCHED_LOAD_SCALE;
+
+		if (max_load - this_load >= busiest_load_per_task * imbn) {
 			*imbalance = busiest_load_per_task;
 			return busiest;
 		}
@@ -2266,10 +2274,6 @@ find_busiest_group(struct sched_domain *
 
 		pwr_now += busiest->cpu_power *
 			min(busiest_load_per_task, max_load);
-		if (this_nr_running)
-			this_load_per_task /= this_nr_running;
-		else
-			this_load_per_task = SCHED_LOAD_SCALE;
 		pwr_now += this->cpu_power *
 			min(this_load_per_task, this_load);
 		pwr_now /= SCHED_LOAD_SCALE;