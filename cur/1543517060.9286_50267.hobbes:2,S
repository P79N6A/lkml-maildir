Date: Fri, 26 Nov 1999 01:56:17 +0100 (CET)
From: Ingo Molnar <>
Subject: Re: [patch] smp-2.3.30-A1, mb(), wmb(), rmb()
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/1999/11/25/154

yep you are right.
On Thu, 25 Nov 1999, Andrea Arcangeli wrote:
> -					_PAGE_DIRTY | _PAGE_ACCESSED | flags)));
> +					_PAGE_DIRTY | _PAGE_ACCESSED | _PAGE_PCD | flags)));
[we definitely need this. Wondering why we werent bitten by this one so
far.]
> +/* IA32 UP enforce strong ordering.
> +
> +   IA32 SMP enforce Processor Ordering on Write Back memory.
(on WB and UC, but hey.)
> +	Stores from one processor may not all be seen by other processors
> +	yet, though if any one store is observed by processor A on
> +	processor B, any earlier stores (in program order) from processor
> +	A are guaranteed to be observed by processor B.
> +
the more important property is that even speculative reads are guaranteed
to 'observe (ie. read) the shared memory image in order'. Note that it's a
valid shared memory model to have everything observable in program order,
but still speculative reads would be allowed to violate causality.
> +#define mb()	barrier()
> +#define rmb()	barrier()
> +#define wmb()	barrier()
(yep you are right, this is the correct variant.)
> -	"lock ; btrl $0,%0"
> +	"movl $0, %0"
done already in pre1-2.3.30, and note that we are using 'movb $0, %0'
because it generates a smaller opcode (less cache footprint).
-- mingo
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.rutgers.edu
Please read the FAQ at 
http://www.tux.org/lkml/