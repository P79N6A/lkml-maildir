Date: Mon, 17 Dec 2007 17:48:32 +0100
From: Ingo Molnar <>
Subject: Re: [PATCH] x86: provide a DMI based port 0x80 I/O delay override.
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2007/12/17/200

* Alan Cox <alan@lxorguk.ukuu.org.uk> wrote:
> I don't think we should be offering udelay based delays at this point. 
> There are a lot of drivers to fix first. This is just one trivial 
> example
> 
> ...
> 
> --- drivers/watchdog/wdt.c~	2007-12-17 15:58:49.000000000 +0000
> +++ drivers/watchdog/wdt.c	2007-12-17 15:58:49.000000000 +0000
> @@ -70,6 +70,8 @@
>  static int io=0x240;
>  static int irq=11;
> 
> +static DEFINE_SPINLOCK(wdt_lock);
> +
>  module_param(io, int, 0);
>  MODULE_PARM_DESC(io, "WDT io port (default=0x240)");
>  module_param(irq, int, 0);
> @@ -109,6 +111,8 @@
> 
>  static int wdt_start(void)
>  {
> +	unsigned long flags;
> +	spin_lock_irqsave(&wdt_lock, flags);
>  	inb_p(WDT_DC);			/* Disable watchdog */
>  	wdt_ctr_mode(0,3);		/* Program CTR0 for Mode 3:
a really stupid question, in what way does:
  inb_p(WDT_DC);
work better than:
  inb(WDT_DC);
  delay(2);
?
(i'm not suggesting you are wrong, this detail just fails to click at 
the moment.)
	Ingo