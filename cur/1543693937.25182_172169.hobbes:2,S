Date: 12 Oct 2003 18:09:16 -0400
From: Greg Stark <>
Subject: Re: statfs() / statvfs() syscall ballsup...
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2003/10/12/90

Linus Torvalds <torvalds@osdl.org> writes:
> > worse, there's no way to indicate that the i/o it's doing is lower priority,
> > so i/o bound servers get hit dramatically. 
> 
> IO priorities are pretty much worthless. It doesn't _matter_ if other 
> processes get preferred treatment - what is costly is the latency cost of 
> seeking. What you want is not priorities, but batching.
What you want depends very much on the circumstances. I'm sure in a lot of
cases batching helps, but in this case it's not the issue.
The vacuum job that runs periodically in fact is batched very well. In fact
that's the main reason it exists rather than having the cleanup handled in the
critical path in the transaction itself. 
I'm not aware of all the details but my understanding is that it reads every
block in the table sequentially, keeping note of all the records that are no
longer visible to any transaction. When it's finished reading it writes out a
"free space map" that subsequent transactions read and use to find available
space in the table.
The vacuum job is makes very efficient use of disk i/o. In fact too efficient.
Frequently people have their disks running at 50-90% capacity simply handling
the random seeks to read data. Those seeks are already batched to the OS's
best ability. 
But then vacuum comes along and tries to read the entire table sequentially.
In the best case the sequential read will take up a lot of the available disk
bandwidth and delay transactions. In the worst case the OS will actually
prefer the sequential read because the elevator algorithm always sees that it
can get more bandwidth by handling it ahead of the random access.
In reality there is no time pressure on the vacuum at all. As long as it
completes faster than dead records can pile up it's fast enough. The
transactions on the other hand must complete as fast as possible.
Certainly batching is useful and in many cases is more important than
prioritizing, but in this case it's not the whole answer.
I'll mention this thread on the postgresql-hackers list, perhaps some of the
more knowledgeable programmers there will have thought about these issues and
will be able to post their wishlist ideas for kernel APIs.
I can see why back in the day Oracle preferred to simply tell all the OS
vendors, "just give us direct control over disk accesses, we'll figure it out"
rather than have to really hash out all the details of their low level needs
with every OS vendor. But between being able to prioritize I/O resources and
cache resources, and being able to sync IDE disks properly and cleanly (that
other thread) Linux may be able drastically improve the kernel interface for
databases.
-- 
greg
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  
http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  
http://www.tux.org/lkml/