Date: Thu, 13 Mar 2008 15:22:39 -0700
From: "Yinghai Lu" <>
Subject: Re: [PATCH] mm: fix boundary checking in free_bootmem_core
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2008/3/13/385

On Thu, Mar 13, 2008 at 2:59 PM, Andi Kleen <ak@suse.de> wrote:
> On Thursday 13 March 2008 02:22:40 Andrew Morton wrote:
>  > On Wed, 12 Mar 2008 18:11:41 -0700 "Yinghai Lu" <yhlu.kernel@gmail.com> wrote:
>  >
>  > > >  <looks at it>
>  > > >
>  > > >  Sorry, but I find the changelog very hard to amke sense of.  I presently
>  > > >  have:
>  > > >
>  > > >
>  > > >   So call it when numa is enabled, we don't know which node have that
>  > > >   range.  and make it more robust.
>  > > >
>  > > >   Try to trim it to get valid sidx, and eidx.
>  > > >
>  > > >  Could you please expand on this?
>  > >
>  > > please check following...
>  > >
>  >
>  > Heaps better, thanks ;)  Below is what I now have.
>  >
>  > (cc's people)
>  >
>  > Guys, could you please review this?  Maybe test it a bit?
>  >
>  > Thanks.
>  >
>  >
>  > From: "Yinghai Lu" <yhlu.kernel@gmail.com>
>  >
>  > With numa enabled, some callers could have a range o fmemory on one node but
>  > try to free that on other node.  This can cause some pages to be freed
>  > wrongly.
>
>  Concrete examples?
>
>  If that happens it's really just a problem that the bootmem API
>  is wrong. I was always annoyed by the hardcoded NODE_DATA(0)s in
>  free_bootmem.
>
>  I would suggest if that happens you just fix free_bootmem to search
>  for the correct node instead of hardcoding 0 and then eliminate
>  free_bootmem_node() everywhere and replace it with free_bootmem()
>
> >
>  > For example: when we try to allocate 128g boot ram early for gart/swiotlb, and
>  > free that range later so gart/swiotlb can get some range afterwards.
>
>  I'm confused by the example. AFAIK there is no memory freeing in either
>  gart nor swiotlb. At least there wasn't until very recently.
For big system when numa=off or disabled, vmemmap will use 3.6g ram
when you have 256g. if you don't allocate the PMD continuous.
then i tried to reserve 64M or 128M RAM before that, and free that
before gart/switotble try to allloc_bootmem under 4g.
that patch will make the system without ram on node0 not happy.
because of free_bootmem is hardcoded to use node0.
>
>
>  >
>  > With this patch, we don't need to care which node holds the range, just loop
>  > to call free_bootmem_node for all online nodes.
>  >
>  > This patch make free_bootmem_core() more robust by trimming the sidx and eidx
>  > according the ram range that the node has.
>
>  I think you should just kill free_bootmem_node() and replace it everywhere
>  with your improved free_bootmem()
using phys_to_nid()? it seems we only have that on x86_64.
also there is assumpation that reserve_bootmem_node, reserver_bootmem
can not cross the nodes.
I want to remove that constrient too.
YH