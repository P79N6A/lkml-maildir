Date: Mon, 26 Jan 2009 22:50:49 +0100
From: Oleg Nesterov <>
Subject: Re: [PATCH 2/3] work_on_cpu: Use our own workqueue.
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2009/1/26/298

On 01/26, Andrew Morton wrote:
>
> On Mon, 26 Jan 2009 22:27:27 +0100
> Ingo Molnar <mingo@elte.hu> wrote:
> 
> > 
> > * Andrew Morton <akpm@linux-foundation.org> wrote:
> > 
> > > > So if it's generic it ought to be implemented in a generic way - not a 
> > > > "dont use from any codepath that has a lock held that might 
> > > > occasionally also be held in a keventd worklet". (which is a totally 
> > > > unmaintainable proposition and which would just cause repeat bugs 
> > > > again and again.)
> > > 
> > > That's different.  The core fault here lies in the keventd workqueue 
> > > handling code.  If we're flushing work A then we shouldn't go and block 
> > > behind unrelated work B.
> > 
> > the blocking is inherent in the concept of "a queue of worklets handled by 
> > a single thread".
> > 
> > If a worklet is blocked then all other work performed by that thread is 
> > blocked as well. So by waiting on a piece of work in the queue, we wait 
> > for all prior work queued up there as well.
> > 
> > The only way to decouple that and to make them independent (and hence 
> > independently flushable) is to create more parallel flows of execution: 
> > i.e. by creating another thread (another workqueue).
> > 
> 
> Nope.  As I said, the caller of flush_work() can detach the work item
> and run it directly.
I am totally confused, just can't understand this thread...
I guess, we can't detach and execute because we can run on the
different CPU.
But "[PATCH 1/3] work_on_cpu: dont try to get_online_cpus() in work_on_cpu."
removes get_online_cpus/put_online_cpus, this means the work can run on
the wrong CPU anyway. Or work_on_cpu() can hang forever if CPU has already
gone away before queue_work_on().
Confused.
Oleg.