Date: Fri, 23 Jan 2009 17:22:31 +0800
From: "Zhang, Yanmin" <>
Subject: Re: [PATCH] SLUB: revert direct page allocator pass through
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2009/1/23/96

On Fri, 2009-01-23 at 10:43 +0200, Pekka J Enberg wrote:
> From: Pekka Enberg <penberg@cs.helsinki.fi>
> 
> This patch reverts page allocator pass-through logic from the SLUB allocator.
> 
> Commit aadb4bc4a1f9108c1d0fbd121827c936c2ed4217 ("SLUB: direct pass through of
> page size or higher kmalloc requests") added page allocator pass-through to the
> SLUB allocator for large sized allocations. This, however, results in a
> performance regression compared to SLAB in the netperf UDP-U-4k test.
> 
> The regression comes from the kfree(skb->head) call in skb_release_data() that
> is subject to page allocator pass-through as the size passed to __alloc_skb()
> is larger than 4 KB in this test. With this patch, the performance regression
> is almost closed:
> 
>   <insert numbers here>
> 
> Reported-by: "Zhang, Yanmin" <yanmin_zhang@linux.intel.com>
> Tested-by: "Zhang, Yanmin" <yanmin_zhang@linux.intel.com>
> Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
> ---
> Yanmin, do you still have the relevant numbers I could cut and paste to 
> the patch description?
I use 2.6.29-rc2 kernel to run netperf UDP-U-4k CPU_NUM client/server pair loopback testing
on x86-64 machines. Comparing with SLUB, SLAB's result is about 2.3 times of SLUB's.
After applying the reverting patch, the result difference between SLUB and SLAB becomes 1%
which we might consider as fluctuation.
yanmin