Date: Thu, 19 Apr 2007 12:18:23 +0200
From: Ingo Molnar <>
Subject: Re: CFS and suspend2: hang in atomic copy (was: [Announce] [patch] Modular Scheduler Core and Completely Fair Scheduler [CFS])
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2007/4/19/146

* Ingo Molnar <mingo@elte.hu> wrote:
> > I think a better approach would be to keep track of the rightmost 
> > entry, set the key to the rightmost's key +1 and then simply insert 
> > it there.
> 
> yeah. I had that implemented at a stage but was trying to be too 
> clever for my own good ;-)
i have fixed it via the patch below. (I'm using rb_last() because that 
way the normal scheduling codepaths are not burdened with the 
maintainance of a rightmost entry.)
	Ingo
---
 kernel/sched.c      |    3 ++-
 kernel/sched_fair.c |   24 +++++++++++++-----------
 2 files changed, 15 insertions(+), 12 deletions(-)
Index: linux/kernel/sched.c
===================================================================
--- linux.orig/kernel/sched.c
+++ linux/kernel/sched.c
@@ -3806,7 +3806,8 @@ asmlinkage long sys_sched_yield(void)
 	schedstat_inc(rq, yld_cnt);
 	if (rq->nr_running == 1)
 		schedstat_inc(rq, yld_act_empty);
-	current->sched_class->yield_task(rq, current);
+	else
+		current->sched_class->yield_task(rq, current);
 
 	/*
 	 * Since we are going to call schedule() anyway, there's
Index: linux/kernel/sched_fair.c
===================================================================
--- linux.orig/kernel/sched_fair.c
+++ linux/kernel/sched_fair.c
@@ -275,21 +275,23 @@ static void dequeue_task_fair(struct rq 
  */
 static void yield_task_fair(struct rq *rq, struct task_struct *p)
 {
+	struct rb_node *entry;
+	struct task_struct *last;
+
 	dequeue_task_fair(rq, p);
 	p->on_rq = 0;
+
 	/*
-	 * Temporarily insert at the last position of the tree:
+	 * Temporarily insert at the last position of the tree.
+	 * The key will be updated back to (near) its old value
+	 * when the task gets scheduled.
 	 */
-	p->fair_key = LLONG_MAX;
+	entry = rb_last(&rq->tasks_timeline);
+	last = rb_entry(entry, struct task_struct, run_node);
+
+	p->fair_key = last->fair_key + 1;
 	__enqueue_task_fair(rq, p);
 	p->on_rq = 1;
-
-	/*
-	 * Update the key to the real value, so that when all other
-	 * tasks from before the rightmost position have executed,
-	 * this task is picked up again:
-	 */
-	p->fair_key = rq->fair_clock - p->wait_runtime + p->nice_offset;
 }
 
 /*
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  
http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  
http://www.tux.org/lkml/