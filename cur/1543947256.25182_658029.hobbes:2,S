Date: Fri, 22 Feb 2008 08:29:20 -0500
From: Gregory Haskins <>
Subject: Re: [PATCH [RT] 05/14] rearrange rt_spin_lock sleep
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2008/2/22/145

Gregory Haskins wrote:
> @@ -732,14 +741,15 @@ rt_spin_lock_slowlock(struct rt_mutex *lock)
> 
>  		debug_rt_mutex_print_deadlock(&waiter);
> 
> -		schedule_rt_mutex(lock);
> +		update_current(TASK_UNINTERRUPTIBLE, &saved_state);
I have a question for everyone out there about this particular part of 
the code. Patch 6/14 adds an optimization that is predicated on the 
order in which we modify the state==TASK_UNINTERRUPTIBLE vs reading the 
waiter.task below.
My assumption is that the xchg() (inside update_current()) acts as an 
effective wmb().  If xchg() does not have this property, then this code 
is broken and patch 6/14 should also add a:
+               smp_wmb();
> +		if (waiter.task)
> +			schedule_rt_mutex(lock);
> +		else
> +			update_current(TASK_RUNNING_MUTEX, &saved_state);
> 
>  		spin_lock_irqsave(&lock->wait_lock, flags);
>  		current->flags |= saved_flags;
>  		current->lock_depth = saved_lock_depth;
> -		state = xchg(&current->state, TASK_UNINTERRUPTIBLE);
> -		if (unlikely(state == TASK_RUNNING))
> -			saved_state = TASK_RUNNING;
Does anyone know the answer to this?
Regards,
-Greg