Date: Fri, 23 Feb 2007 16:58:26 -0500
From: "Dmitry Torokhov" <>
Subject: Re: [KJ][RFC][PATCH] BIT macro cleanup
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2007/2/23/259

On 2/23/07, Richard Knutsson <ricknu-0@student.ltu.se> wrote:
> Dmitry Torokhov wrote:
> >
> > Hm, I thought as was clear, but apparently I messed up explaining my
> > position:
> >
> > 1. I don't like BITWRAP name at all and I don't want anything like
> > that near input code. I think BIT is just fine.
> Oh, I think I understand now. So the (in input.h):
> #undef BIT
> #define BIT(...
> business is what you want to do? Well, that I will not object to.
No, #undefs may be barely tolerable in .c files but they are not
acceptable in core subsystem interfaces. If you do that you will never
know what version of BIT patricular module is using.
>  Your
> patch with:
> +#define BIT(nr)        (1UL << (nr))
> +#define LLBIT(nr) (1ULL << (nr))
> +#define BITWRAP(nr)    (1UL << ((nr) % BITS_PER_LONG))
> in bitops.h made me believe the #undef in input.h was just a temporarily
> thing.
No. There is no "my patch". You are confusing me with Milind
Choudhary. I am saying that IMO input's BIT definition should be
adequate for 99% of potential users and that I would be OK with moving
said BIT definition from input.h to bitops.h and maybe supplementing
it with LLBIT. I am also saying that I do not want BITWRAP, BITSWAP
(what swap btw?) nor BIT(x % BITS_PER_LONG) in input drivers.
-- 
Dmitry
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  
http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  
http://www.tux.org/lkml/