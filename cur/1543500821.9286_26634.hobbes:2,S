Date: Mon, 28 Jun 1999 00:48:45 -0600
From: Larry McVoy <>
Subject: Re: FTP benchmark proposal
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/1999/6/28/14

: Andi> The hard thing will be to simulate slow/timing out connections
: Andi> that occur over the internet. Just low-latency local networking
: Andi> with no lost packets is easy.
: 
: Good point, we are able to sustain 32MB/sec when reading data off four
: SCSI tape drives in parallel and sending data over the wire via TCP on
: Gigabit Ethernet on a dual 450MHz Linux box. I am quite certain Linux
: will be able to cope under good LAN like conditions as you say, the
: task is to setup a realistic test which people will accept. Of course
: using ftp does itself help mess things up ;-)
So I think that a point worht bring out here is that most of these
connections are not fast - many of them are behind modems or DSL lines
which top out at 5 - 50KB/sec each.  In fact, if you do the math -
if they had 6000 clients all transfering at the same time, then the
bandwidth is 2.9KB/sec/client.
It's a much harder problem to pump 16MB/sec through 6000 sockets at
3KB/sec/socket.  One socket is relatively easy by comparison.
So I wouldn't worry too much about making the test realistic.  if you
can set up a work load that has 6000 sockets going at the server in
parallel, I suspect that it will stress the server just fine.  It's when
you try to do the load through a few sockets that all the timing enters
into the equation.  Yeah, I'm sure the tests will have to be played
with a bit, but the first step is to just do it and see what we get.
I'll call VA tomorrow and see if they are interested.  Red Hat is also
setting up such a lab on the East Coast.
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.rutgers.edu
Please read the FAQ at 
http://www.tux.org/lkml/