Date: Mon, 28 Feb 2000 11:21:49 -0500 (EST)
From: William Montgomery <>
Subject: Re: lowlatency-2.2.14-B1 + 2.2.14aa7 fixes crash, but...
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2000/2/28/82

On Thu, 24 Feb 2000, Ingo Molnar wrote:
> > > something like:
> > > 
> > >  +                       if (current->need_resched) {
> > >  +                               INODE(tmp)->i_count++;
> > >  +                               spin_unlock(&inode_lock);
> > >  +                               schedule();
> > >  +                               iput(tmp);
> > >  +                               goto repeat;
> > > 
> > > is there any problem with this?
> > > 
I fixed __free_inodes as suggested by Ingo, all seemed fine during
10 hours of testing.  Then I got a crash/lockup and had to reboot.
Last entry in syslog:
Feb 26 04:41:19 vmebox kernel: iput: device 00:00 inode -2119956477 still has aliases!
Feb 26 04:41:19 vmebox kernel: iput: Aieee, semaphore in use inode 00:00/-2119956477, count=0
Feb 26 04:41:19 vmebox kernel: iput: Aieee, atomic write semaphore in use inode 00:00/-2119956477, count=0
Feb 26 04:41:19 vmebox kernel: iput: device 00:00 inode -2119956477 still has aliases!
Feb 26 04:41:19 vmebox kernel: iput: Aieee, semaphore in use inode 00:00/-2119956477, count=0
Feb 26 04:41:19 vmebox kernel: iput: Aieee, atomic write semaphore in use inode 00:00/-2119956477, count=0
Feb 26 04:41:19 vmebox kernel: iput: device 00:00 inode -2119956477 still has aliases!
Feb 26 04:41:19 vmebox kernel: iput: Aieee, semaphore in use inode 00:00/-2119956477, count=0
Feb 26 04:41:19 vmebox kernel: iput: Aieee, atomic write semaphore in use inode 00:00/-2119956477, count=0
Feb 26 04:41:19 vmebox kernel: iput: device 00:00 inode -2119956477 still has aliases!
I think I mangled something, the code now looks like this:
-----------------------------------------
        int found = 0, count = 128;
        int repeat_count = 1;
repeat:
        entry = inode_in_use.next;
        while (entry != &inode_in_use) {
                struct list_head *tmp = entry;
                /*
                 * Tough one to time-limit ...
                 */
                entry = entry->next;
                if (!CAN_UNUSE(INODE(tmp))) {
                        if (current->need_resched) {
                                INODE(tmp)->i_count++;
                                spin_unlock(&inode_lock);
                                schedule();
                                iput(tmp);
                                if (repeat_count) {
                                        repeat_count--;
                                        goto repeat;
                                }
                        }
                        continue;
                }
                list_del(tmp);
                list_del(&INODE(tmp)->i_hash);
                INIT_LIST_HEAD(&INODE(tmp)->i_hash);
                list_add(tmp, freeable);
                list_entry(tmp, struct inode, i_list)->i_state = I_FREEING;
                found++;
                if (!--count)
                        break;
        }
        return found;
}
------------------------
It appears Andrea has put in a lowlatency enhancement already in this
code by only allowing 128 inodes to be freed at a time.  Perhaps we
dont need the conditional schedule at all?  Any rough idea about how long
it takes to free 128 inodes?
Wm
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.rutgers.edu
Please read the FAQ at 
http://www.tux.org/lkml/