Date: Thu, 25 Nov 1999 11:51:26 +0100 (CET)
From: Ingo Molnar <>
Subject: [patch] smp-2.3.30-A1, mb(), wmb(), rmb()
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/1999/11/25/16

the attached patch completes the memory-ordering changes started by the
spin_unlock() change in 2.3.30-pre1. Here is the gist of the patch:
/*
 * Force strict CPU ordering.
 * And yes, this is required on UP too when we're talking
 * to devices. Note that these primitives deal with operation
 * ordering, not atomicity.
 *
 * For now, "wmb()" doesn't actually do anything, as all
 * Intel CPU's follow what Intel calls a *Processor Order*,
 * in which all writes are seen in the program order even
 * outside the CPU.
 *
 * rmb(): writes are guaranteed by Intel to never pass reads
 * (in an externally visible way that is), so the read barrier
 * is just a dummy store. Note that (%%esp) is ours and only
 * ours, so this is IRQ-safe as well. The barrier _must_
 * include a store, thus the assembly. The CPU snoops external
 * devices (including CPUs and DMA agents), so this works out
 * just fine in all cases and is lightweight. And this is only
 * 4 bytes icache footprint total. 'addl $0, (%%esp)' is a legal
 * write operation which dirties the target cacheline.
 *
 * We expect future Intel CPU's to have a weaker ordering. But
 * it's not all that bad right now already.
 */
#define mb()    __asm__ __volatile__ ("addl $0, (%%esp)": : :"memory")
#define rmb()   mb()
#define wmb()   __asm__ __volatile__ ("": : :"memory")
#define set_rmb(var, value) do { var = value; rmb(); } while (0)
#define set_mb(var, value) set_rmb(var, value)
#define set_wmb(var, value) do { var = value; wmb(); } while (0)
The patch works just fine on my 8-way.
-- mingo
--- linux/include/asm-i386/system.h.orig	Fri Oct 15 18:29:42 1999
+++ linux/include/asm-i386/system.h	Thu Nov 25 02:34:57 1999
@@ -161,21 +161,31 @@
 /*
  * Force strict CPU ordering.
  * And yes, this is required on UP too when we're talking
- * to devices.
+ * to devices. Note that these primitives deal with operation
+ * ordering, not atomicity.
  *
  * For now, "wmb()" doesn't actually do anything, as all
  * Intel CPU's follow what Intel calls a *Processor Order*,
  * in which all writes are seen in the program order even
  * outside the CPU.
  *
- * I expect future Intel CPU's to have a weaker ordering,
- * but I'd also expect them to finally get their act together
- * and add some real memory barriers if so.
+ * rmb(): writes are guaranteed by Intel to never pass reads
+ * (in an externally visible way that is), so the read barrier
+ * is just a dummy store. Note that (%%esp) is ours and only
+ * ours, so this is IRQ-safe as well. The barrier _must_
+ * include a store, thus the assembly. The CPU snoops external
+ * devices (including CPUs and DMA agents), so this works out
+ * just fine in all cases and is lightweight. And this is only
+ * 4 bytes icache footprint total. 'addl $0, (%%esp)' is a legal
+ * write operation which dirties the target cacheline.
+ *
+ * We expect future Intel CPU's to have a weaker ordering. But
+ * it's not all that bad right now already.
  */
-#define mb() 	__asm__ __volatile__ ("lock; addl $0,0(%%esp)": : :"memory")
+#define mb() 	__asm__ __volatile__ ("addl $0, (%%esp)": : :"memory")
 #define rmb()	mb()
 #define wmb()	__asm__ __volatile__ ("": : :"memory")
-#define set_rmb(var, value) do { xchg(&var, value); } while (0)
+#define set_rmb(var, value) do { var = value; rmb(); } while (0)
 #define set_mb(var, value) set_rmb(var, value)
 #define set_wmb(var, value) do { var = value; wmb(); } while (0)
 