Date: Wed, 20 Feb 2008 22:39:32 +0100
From: Dmitry Adamushko <>
Subject: [PATCH 2/2] kthread: call wake_up_process() whithout the lock being held
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2008/2/20/554

From: Dmitry Adamushko <dmitry.adamushko@gmail.com>
Subject: kthread: call wake_up_process() whithout the lock being held
- from the POV of synchronization, there should be no need to call wake_up_process()
with the 'kthread_create_lock' being held;
- moreover, in order to support a lockless check for list_empty(&kthread_create_list)
in kthreadd() :
 	set_current_state(TASK_INTERRUPTIBLE);
    	if (list_empty(&kthread_create_list))
    		schedule();
    
we must ensure that a modification of the list (i.e. list_add_tail()) has been completed
by the moment a state of the task is checked in try_to_wake_up().
i.e. they must not be re-ordered.
    
wake_up_process() (i.e. try_to_wake_up() effectively) doesn't provide a full mb.
By moving wake_up_process() out of the locked section, we get an UNLOCK/LOCK
pair (LOCK is in try_to_wake_up()) which is guaranteed to act as a full mb.
    
Signed-off-by: Dmitry Adamushko <dmitry.adamushko@gmail.com>
diff --git a/kernel/kthread.c b/kernel/kthread.c
index d7a7897..ec68e0f 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -158,9 +158,9 @@ struct task_struct *kthread_create(int (*threadfn)(void *data),
 
 	spin_lock(&kthread_create_lock);
 	list_add_tail(&create.list, &kthread_create_list);
-	wake_up_process(kthreadd_task);
 	spin_unlock(&kthread_create_lock);
 
+	wake_up_process(kthreadd_task);
 	wait_for_completion(&create.done);
 
 	if (!IS_ERR(create.result)) {