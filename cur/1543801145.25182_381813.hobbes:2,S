Date: Thu, 05 Jan 2006 14:21:50 +1100
From: Nick Piggin <>
Subject: Re: [patch 00/21] mutex subsystem, -V14
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2006/1/4/515

Linus Torvalds wrote:
> 
> On Wed, 4 Jan 2006, Nicolas Pitre wrote:
> 
> 
>>On Wed, 4 Jan 2006, Joel Schopp wrote:
>>
>>
>>>>this is version 14 of the generic mutex subsystem, against v2.6.15.
>>>>
>>>>The patch-queue consists of 21 patches, which can also be downloaded from:
>>>>
>>>>  
http://redhat.com/~mingo/generic-mutex-subsystem/
>>>>
>>>
>>>Took a glance at this on ppc64.  Would it be useful if I contributed an arch
>>>specific version like arm has?  We'll either need an arch specific version or
>>>have the generic changed.
>>
>>Don't change the generic version.  You should provide a ppc specific 
>>version if the generic ones don't look so good.
> 
> 
> Well, if the generic one generates _buggy_ code on ppc64, that means that 
> either the generic version is buggy, or one of the atomics that it uses is 
> buggily implemented on ppc64.
> 
> And I think it's the generic mutex stuff that is buggy. It seems to assume 
> memory barriers that aren't valid to assume.
> 
> A mutex is more than just updating the mutex count properly. You also have 
> to have the proper memory barriers there to make sure that the things that 
> the mutex _protects_ actually stay inside the mutex.
> 
> So while a ppc64-optimized mutex is probably a good idea per se, I think 
> the generic mutex code had better be fixed first and regardless of any 
> optimized version.
> 
> On x86/x86-64, the locked instructions automatically imply the proper 
> memory barriers, but that was just lucky, I think.
> 
I think the generic code is correct according to Documentation/atomic_ops.txt
which basically defines any atomic_xxx operation which both modifies its
operand and returns something to have a full memory barrier before and after
its load/store operations.
Side note, why can't powerpc use lwsync for smp_wmb? The only problem seems to
be that it allows loads to be reordered before stores, but that's OK with
smp_wmb, right?
And why is smp_wmb() (ie. the non I/O barrier) doing eieio, while wmb() does
not? And rmb() does lwsync, which apparently does not order IO at all...
-- 
SUSE Labs, Novell Inc.
Send instant messages to your online friends 
http://au.messenger.yahoo.com
 
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  
http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  
http://www.tux.org/lkml/