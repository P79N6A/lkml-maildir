Date: Thu, 7 Sep 2006 10:10:51 +0100
From: "Catalin Marinas" <>
Subject: Re: [PATCH 2.6.18-rc6 00/10] Kernel memory leak detector 0.10
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2006/9/7/36

On 07/09/06, Michal Piotrowski <michal.k.k.piotrowski@gmail.com> wrote:
> On 07/09/06, Catalin Marinas <catalin.marinas@gmail.com> wrote:
> > On 07/09/06, Michal Piotrowski <michal.k.k.piotrowski@gmail.com> wrote:
> > > On 07/09/06, Catalin Marinas <catalin.marinas@gmail.com> wrote:
> > > > On 07/09/06, Michal Piotrowski <michal.k.k.piotrowski@gmail.com> wrote:
> > > > > CONFIG_DEBUG_MEMLEAK=y
> > > > > CONFIG_DEBUG_MEMLEAK_HASH_BITS=8
> >
> > Have you tried 16?
>
> No, I haven't.
8 hash bits would lead to a really slow hash table lookup since you
would only have 256 entries and it uses linked lists to deal with
collisions (you may have tens of thousands of pointers to be stored in
the hash). Anyway, I attach a patch which allows you to set small
values but it is highly unrecommended.
-- 
Catalin
Allow small values of HASH_BITS to be configured
From: Catalin Marinas <catalin.marinas@arm.com>
The original implementation was assuming that the HASH_BITS value is small
big enough so that the page order is at least 0. This patch corrects this.
Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
---
 mm/memleak.c |    9 ++++++---
 1 files changed, 6 insertions(+), 3 deletions(-)
diff --git a/mm/memleak.c b/mm/memleak.c
index 0b5aa0c..a65d930 100644
--- a/mm/memleak.c
+++ b/mm/memleak.c
@@ -180,12 +180,15 @@ #define OBJECT_TYPE_GUESSED	0x2
 /* Hash functions */
 static void hash_init(void)
 {
-	int i;
-	int hash_size = sizeof(*pointer_hash) * (1 << HASH_BITS);
-	int hash_order = fls(hash_size) - 1;
+	unsigned int i;
+	unsigned int hash_size = sizeof(*pointer_hash) * (1 << HASH_BITS);
+	unsigned int hash_order = fls(hash_size) - 1;
 
+	/* hash_size not a power of 2 */
 	if (hash_size & ((1 << hash_order) - 1))
 		hash_order += 1;
+	if (hash_order < PAGE_SHIFT)
+		hash_order = PAGE_SHIFT;
 
 	pointer_hash = (struct hlist_head *)
 		__get_free_pages(GFP_ATOMIC, hash_order - PAGE_SHIFT);