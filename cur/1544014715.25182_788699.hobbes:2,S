Date: Thu, 08 Jan 2009 00:04:28 -0800
From: "H. Peter Anvin" <>
Subject: Re: [PATCH] [3/5] Mark complex bitops.h inlines as __always_inline
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2009/1/8/38

Ingo Molnar wrote:
> * Hugh Dickins <hugh@veritas.com> wrote:
> 
>> Hugh Dickins noticed that released gcc versions building the kernel with 
>> CONFIG_OPTIMIZE_INLINING=y don't inline some of the bitops - sometimes 
>> generating very inefficient pageflag tests, and many instances of 
>> constant_test_bit().
> 
> Could you quantify that please?
> 
> We really dont want to reintroduce __always_inline just for performance / 
> code size reasons. If GCC messes up and makes a larger / more inefficient 
> kernel, GCC will be fixed. CONFIG_OPTIMIZE_INLINING is default-off, so 
> enable it only if it improves your kernel.
> 
There is one condition under which gcc simply won't know, and that is
when an inline is composed primarily of asm code.  gcc, I believe,
creates a worst-case estimate based on the number of semicolons or
newlines (something that works semi-okayish on RISC), and thus tend to
vastly overestimate the size of an asm() on x86, where statements are
highly variable length.  Hence it is probably always going to need
hints, unless the whole handling of inline assembly is revamped (which
would be good for scheduling, but I doubt it will happen.)
	-hpa