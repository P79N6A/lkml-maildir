Date: Sun, 7 Nov 1999 18:19:14 +0100
From: Florian Lohoff <>
Subject: Raid/LVM ll_rw_blk problems Got xx request  Was: (Re: LOTS OF BAD STUFF ...)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/1999/11/7/60

On Sat, Nov 06, 1999 at 08:40:08PM +0100, P.A.M. van Dam wrote:
> Same here, but then with Mauelschagens LVM. An mkfs -t ext2 for the
> first time (after bootup) on a striped LV will trigger the "Got LVM
> request, not good" message (and accompanied by some "trying to ...
> beyond end of device" messages). The second attempt will not generate
> these messages.
> 
> I'm sorry, but I still have the strong belief that this isn't hardware
> nor RAID (only) related. There must be some bug lurking, we have to
> stamp out before we can take 2.4 seriously.
I heard of that - I meat Heinz in Siegen where we already discussed this
topic and i thought he had given me enough direction and/or infos
to debug this topic on my own. I spent 2 days of hacking resulting in NULL.
> > I reproduced this with different hardware (machines, harddrives,
> > scsi controllers etc - Even on sparc ...)
> 
> Ok. Does SMP be the common denominator in your test? Almost all reports
> are with SMP machines.
Nope - Single Processor Sparc, Single Processor i386 (P200) and a Dual
Celeron 400 ... Sparc with Qlogic Controller, P200 with 2x Narrow NCR875
aka Symbios Controller and the Dual Celeron with a Ultra Wide SCSI Controller
(Some NCR chip also - Tekram UW Controller)
Although i had a couple of other different problems on Sparc.
It isnt Hardware specific. I only saw that with Raid code although
Heinz meant that he seens this under Heavy load in the LVM also ... 
So this seems to be a common thing between the different ll_rw_blk
usages. I discovered this in the 2.2. area (10-13).
I am not used to kernel things very much - I see the error when i have
the already mentioned setup. I then initiate the "mke2fs" - It displays
writing a couple of superblock copies and in the moment the drives (Up to 12)
should begin to write - Nothing happens and instead i get an "Got md request".
So it seems it happens in the moment the request queue/drive gets
"unplugged" (Am i right with that ?)
Flo
-- 
Florian Lohoff		flo@rfc822.org		      	+49-5241-470566
  ...  The failure can be random; however, when it does occur, it is
  catastrophic and is repeatable  ...             Cisco Field Notice
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.rutgers.edu
Please read the FAQ at 
http://www.tux.org/lkml/