Date: Sat, 2 Nov 2002 15:08:16 -0200
From: Denis Vlasenko <>
Subject: Re: [PATCH] 2/2 2.5.45 cleanup & add original copy_ro/from_user
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2002/11/2/69

On 2 November 2002 10:09, Andi Kleen wrote:
> > That depends on size. If you do huge memcpy (say 1 mb) it still
> > wins by wide margin. Not that we do such huge operations often,
> > but code can check size and pick different routines for small
> > and big blocks
>
> The kernel nevers does such huge memcpys. It rarely does handle any
> buffer bigger than a page (4K)
Okay I take that.
How did you determined that movntXX stores are net loss?
I thought about that and didn't came to a working solution.
It's easy to time memcpy() but harder to measure susequent
cache misses when copied data gets accessed. We can read it
back after memcpy and measure memcpy()+read, but is entire
copy gets used immediately after memcpy() in real world usage?
We're in benchmarking hell :(
--
vda
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  
http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  
http://www.tux.org/lkml/