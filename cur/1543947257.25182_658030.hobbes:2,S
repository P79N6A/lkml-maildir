Date: Fri, 22 Feb 2008 08:35:14 -0500 (EST)
From: Steven Rostedt <>
Subject: Re: [PATCH [RT] 05/14] rearrange rt_spin_lock sleep
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2008/2/22/146

On Fri, 22 Feb 2008, Gregory Haskins wrote:
> Gregory Haskins wrote:
> > @@ -732,14 +741,15 @@ rt_spin_lock_slowlock(struct rt_mutex *lock)
> >
> >  		debug_rt_mutex_print_deadlock(&waiter);
> >
> > -		schedule_rt_mutex(lock);
> > +		update_current(TASK_UNINTERRUPTIBLE, &saved_state);
>
> I have a question for everyone out there about this particular part of
> the code. Patch 6/14 adds an optimization that is predicated on the
> order in which we modify the state==TASK_UNINTERRUPTIBLE vs reading the
> waiter.task below.
>
> My assumption is that the xchg() (inside update_current()) acts as an
> effective wmb().  If xchg() does not have this property, then this code
> is broken and patch 6/14 should also add a:
>
>
> +               smp_wmb();
I believe that the wmb would be needed. I doubt that xchg on all archs
would force any ordering of reads and writes. It only needs to guarantee the
atomic nature of the data exchange. I don't see any reason that it would
imply any type of memory barrier.
-- Steve
>
>
> > +		if (waiter.task)
> > +			schedule_rt_mutex(lock);
> > +		else
> > +			update_current(TASK_RUNNING_MUTEX, &saved_state);
> >
> >  		spin_lock_irqsave(&lock->wait_lock, flags);
> >  		current->flags |= saved_flags;
> >  		current->lock_depth = saved_lock_depth;
> > -		state = xchg(&current->state, TASK_UNINTERRUPTIBLE);
> > -		if (unlikely(state == TASK_RUNNING))
> > -			saved_state = TASK_RUNNING;
>
>
> Does anyone know the answer to this?
>
> Regards,
> -Greg
>