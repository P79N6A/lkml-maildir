Date: 15 Feb 1999 05:44:08 +0100
From: (Matthias Urlichs)
Subject: Re: fsync on large files
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/1999/2/14/61

Olaf Titz <olaf@bigred.inka.de> writes:
> > The real fix for this problem would be to use a syslogd which uses datagram
> > sockets instead. (Both libc5 and glibc understand them.)
> 
> Which means, when syslogd is really overloaded, silently throwing away
> half of the messages. Hmph.
The current way of doing things is worse, because I can cause syslogd to
silently throw away _all_ messages (from processes which haven't syslogged
anything before).
Simply open /dev/log until you run out of file descriptors. Repeat that
until you run out of process slots. If that still falls short (some sites
do have restrictive ulimits), connect to an inetd-based service and cause
its server to syslog something (but not die); I assume there are servers
which can be convinced to do this.
Repeat until successful.
If you're concerned about lossage, then make unix-domain sockets nonblocking.
Shouldn't be too difficult...
-- 
Matthias Urlichs  |  noris network GmbH   |   smurf@noris.de  |  ICQ: 20193661
The quote was selected randomly. Really.    |      
http://www.noris.de/~smurf/
-- 
In the factory, we make cosmetics.  In the store, we sell hope.
                                       -- Charles Revson
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.rutgers.edu
Please read the FAQ at 
http://www.tux.org/lkml/