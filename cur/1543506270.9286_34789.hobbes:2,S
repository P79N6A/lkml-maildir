Date: Wed, 18 Aug 1999 19:39:37 +0100 (BST)
From: Alan Cox <>
Subject: Re: [bigmem-patch] 4GB with Linux on IA32
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/1999/8/18/106

> which is this thing Intel has been flogging for a while.  The API
> allows user processes to register arbitrary virtual address ranges for
> DMA; the driver then pins the pages and loads a map on the adapter
> with virtual->user translations so that the user process can initiate
> DMA using virtual addresses.
You have the logic backwards. The way you are operating is almost ok but
instead of mapping user pages into the kernel and pinning them map kernel
pages into user space and voila, end of problems
(BTW I have an example driver that does this for PCI dmaable space from
user mode if you want it)
> and never to kernel virtual memory addresses.  The mmap() solution
> doesn't work here, at least in a way that conforms with the API we're
> trying to implement, because it doesn't allow the application to
> register its own buffers.  Physical contiguousness isn't an issue,
> because the hardware can do per-page scatter/gather.
I suspect mmap does fit the API. VI requires page alignment if I remember
so its just that your VI code needs to do a sneaky mmap when I register
the buffer memory.
> about it.  However, it's quite possible that translating user virtual
> addresses to bus addresses and locking the pages would be useful in
> other cases, like zero-copy transport protocols and possibly in future
> async I/O, although in those cases the mapping&locking would only last
> for the duration of a single I/O operation.
Have a look at Stephen's rawio patches. 
Alan
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.rutgers.edu
Please read the FAQ at 
http://www.tux.org/lkml/