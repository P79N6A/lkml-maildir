Date: Thu, 21 Feb 2008 18:59:08 +0530
From: Balbir Singh <>
Subject: Re: Make yield_task_fair more efficient
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2008/2/21/143

Mike Galbraith wrote:
> Hi,
> 
> On Thu, 2008-02-21 at 15:01 +0530, Balbir Singh wrote:
>> Ingo Molnar wrote:
>>> * Balbir Singh <balbir@linux.vnet.ibm.com> wrote:
>>>
>>>> If you insist that sched_yield() is bad, I might agree, but how does 
>>>> my patch make things worse. [...]
>>> it puts new instructions into the hotpath.
>>>
>>>> [...] In my benchmarks, it has helped the sched_yield case, why is 
>>>> that bad? [...]
>>> I had the same cache for the rightmost task in earlier CFS (it's a 
>>> really obvious thing) but removed it. It wasnt a bad idea, but it hurt 
>>> the fastpath hence i removed it. Algorithms and implementations are a 
>>> constant balancing act.
>> This is more convincing, was the code ever in git? How did you measure the
>> overhead?
> 
> Counting enqueue/dequeue cycles on my 3GHz P4/HT running a 60 seconds
> netperf test that does ~85k/s context switches  shows:
> 
> sched_cycles: 7198444348 unpatched
> vs
> sched_cycles: 8574036268 patched
Thanks for the numbers! I am very convinced that the patch should stay out until
we can find a way to reduce the overhead. I'll try your patch and see what the
numbers look like as well.
-- 
	Warm Regards,
	Balbir Singh
	Linux Technology Center
	IBM, ISTL