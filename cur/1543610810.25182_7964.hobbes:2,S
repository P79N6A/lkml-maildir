Date: Mon, 26 Nov 2001 14:19:14 -0500
From: Benjamin LaHaise <>
Subject: Re: [PATCH] Scalable page cache
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2001/11/26/171

On Mon, Nov 26, 2001 at 09:40:53PM +0100, Ingo Molnar wrote:
> this does not appear to be the case (see my other replies). Even if the
> hash table is big and assuming the worst-case (we miss on every hash table
> access), mem_map is *way* bigger in the cache because it has a much less
> compressed format. The compression ratio between mem_map[] and the hash
> table is 1:8 in the stock kernel, 1:4 with the page buckets patch.
Well, the only point you've made that has any impact on the data structure 
that I'm proposing is that the cachling bouncing caused by lock acquisition 
is the limiting factor.  The best solution to that is to make the per-bucket 
lock only used for insert and remove, and make lookups lockless.  In order 
to make that work, we need to break the current PageLock into a spinlock 
portion and an io-lock flag, and grab the spinlock before removing the page 
from the page cache.  Now, would you care to comment on the data structure?
		-ben
-- 
Fish.
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  
http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  
http://www.tux.org/lkml/