Date: Wed, 16 Mar 2005 11:26:03 +0100
From: Ingo Molnar <>
Subject: Re: [patch 0/3] j_state_lock, j_list_lock, remove-bitlocks
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2005/3/16/52

* Steven Rostedt <rostedt@goodmis.org> wrote:
> > > ooh, I'd rather not.  I spent an intense three days removing all the
> > > sleeping locks from ext3 (and three months debugging the result).
> > > Ended up gaining 1000% on 16-way.
> > >
> > > Putting them back in will really hurt the SMP performance.
> >
> > ah. Yeah. Sniff.
> >
> > if we gain 1000% on a 16-way then there's something really wrong about
> > semaphores (or scheduling) though. A semaphore is almost a spinlock, in
> > the uncontended case - and even under contention we really (should) just
> > spend the cycles that we'd spend spinning. There will be some
> > intermediate contention level where semaphores hurt, but 1000% sounds
> > truly excessive.
> >
> 
> Could it possibly be that in the process of removing all the sleeping
> locks from ext3, that Andrew also removed a flaw in ext3 itself that
> is responsible for the 1000% improvement?
i think the chances for that are really remote. I think it must have
been a workload ending up scheduling itself to death, while spinlocks
force atomicity of execution and affinity.
we should be able to see the same scenario with PREEMPT_RT on a 16-way
:-)
	Ingo
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  
http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  
http://www.tux.org/lkml/