Date: Mon, 23 Jan 2006 13:40:46 -0500
From: 	Valdis.Kletnieks@vt ...
Subject: Re: [RFC] VM: I have a dream...
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2006/1/23/184

On Mon, 23 Jan 2006 21:03:06 +0300, Al Boldi said:
> The idea here is to run inside swap instead of using it as an addon.
> In effect running inside memory cached by physical RAM.
> 
> Wouldn't something like this at least represent a simple starting point?
We *already* treat RAM as a cache for the swap space and other backing store
(for instance, paging in executable code from a file), if you're looking at
it from the 30,000 foot fly-over...
However, it quickly digresses from a "simple starting point" when you try to
get decent performance out of it, even when people are doing things that tend
to make your algorithm fold up.  A machine with a gigabyte of memory has on the
order of a quarter million 4K pages - which page are you going to move out to
swap to make room?  And if you guess wrong, multiple processes will stall as
the system starts to thrash. (In fact, "thrashing" is just a short way of
saying "consistently guessing wrong as to which pages will be needed soon"....)
But hey, if you got a new page replacement algorithm that performs better,
feel free to post the code.. ;)
Example of why it's a pain in the butt:
A process does a "read(foo, &buffer, 65536);".  buffer is declared as 16
contiguous 4K pages, none of which are currently in memory.  How many pages do
you have to read in, and at what point do you issue the I/O? (hint - work this
problem for a device that's likely to return 64K of data, and again for a
device that has a high chance of only returning 2K of data.....)
But yeah, other than all the cruft like that, it's simple. :)
[unhandled content-type:application/pgp-signature]