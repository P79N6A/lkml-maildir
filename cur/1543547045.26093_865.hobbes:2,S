Date: Tue, 29 Aug 2000 13:02:02 +0200 (CEST)
From: Ingo Molnar <>
Subject: [patch] sched-2.4.0-test8-B0
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2000/8/29/142

On Tue, 29 Aug 2000, David S. Miller wrote:
>    i agree fully. The attached patch against test8-pre1 changes
>    schedule_idle() from inline to FASTCALL - things look much nicer with this
>    one applied.
> 
> Ingo, the local flags stuff on Sparc, remember?
ouch, i thought this has been taken care of. The attached
sched-2.4.0-test8-B0 patch gets rid of flags-passing in reschedule_idle().
(the patch is relative to test8-pre1 + the previous scheduler patch) While
i liked smp_send_reschedule() being executed in parallel (it stresses the
APIC code harder), putting it into the critical section is just a small
increase of critical section length and the code gets cleaner (and
slightly faster) as a result. Patch applies, compiles, works here just
fine. This should solve your problem, right?
	Ingo
--- linux/kernel/sched.c.orig	Tue Aug 29 12:30:18 2000
+++ linux/kernel/sched.c	Tue Aug 29 12:33:32 2000
@@ -213,9 +213,9 @@
  * This function must be inline as anything that saves and restores
  * flags has to do so within the same register window on sparc (Anton)
  */
-static FASTCALL(void reschedule_idle(struct task_struct *, unsigned long));
+static FASTCALL(void reschedule_idle(struct task_struct * p));
 
-static void reschedule_idle(struct task_struct * p, unsigned long flags)
+static void reschedule_idle(struct task_struct * p)
 {
 #ifdef CONFIG_SMP
 	int this_cpu = smp_processor_id();
@@ -286,7 +286,6 @@
 		goto preempt_now;
 	}
 
-	spin_unlock_irqrestore(&runqueue_lock, flags);
 	return;
 		
 send_now_idle:
@@ -298,12 +297,10 @@
 	if ((tsk->processor != current->processor) && !tsk->need_resched)
 		smp_send_reschedule(tsk->processor);
 	tsk->need_resched = 1;
-	spin_unlock_irqrestore(&runqueue_lock, flags);
 	return;
 
 preempt_now:
 	tsk->need_resched = 1;
-	spin_unlock_irqrestore(&runqueue_lock, flags);
 	/*
 	 * the APIC stuff can go outside of the lock because
 	 * it uses no task information, only CPU#.
@@ -318,7 +315,6 @@
 	tsk = cpu_curr(this_cpu);
 	if (preemption_goodness(tsk, p, this_cpu) > 1)
 		tsk->need_resched = 1;
-	spin_unlock_irqrestore(&runqueue_lock, flags);
 #endif
 }
 
@@ -367,9 +363,7 @@
 	if (task_on_runqueue(p))
 		goto out;
 	add_to_runqueue(p);
-	reschedule_idle(p, flags); // spin_unlocks runqueue
-
-	return;
+	reschedule_idle(p);
 out:
 	spin_unlock_irqrestore(&runqueue_lock, flags);
 }
@@ -482,10 +476,9 @@
 	 * current process as well.)
 	 */
 running_again:
-	if (prev == idle_task(smp_processor_id()))
-		goto out_unlock;
-	reschedule_idle(prev, flags); // spin_unlocks runqueue
-	return;
+	if (prev != idle_task(smp_processor_id()))
+		reschedule_idle(prev);
+	goto out_unlock;
 #endif /* CONFIG_SMP */
 }
 