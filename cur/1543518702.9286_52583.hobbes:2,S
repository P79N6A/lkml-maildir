Date: Fri, 10 Dec 1999 08:33:02 -0500 (EST)
From: Ingo Molnar <>
Subject: Re: mmap on a device returns ENODEV
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/1999/12/10/67

the attached small patch against 2.3.32-pre2 adds all pagecache blocks
that have established mappings to the buffer-cache hashlists (and can thus
can be looked up), including the swapcache. It works fine here and there
is no noticeable slowdown anywhere. 
this does not fully solve the problem yet, what we need now is an agreed
on set of rules to access buffers that are in other caches as well (eg.
the pagecache). 
Is it bad to synchronize access through the buffer-cache? I dont think so
and it's simple and straightforward. The disadvantage is that the
pagecache has to maintain the state of the buffer properly, which adds
some overhead. Eg if. a page sees a pagefault with write-access then all
underlying buffers have to be marked as 'permanent dirty'. Permament dirty
buffers are not written back by bdflush. This very same new buffer-state
could i believe be used by the journalling code too to tell other cache
users that a block is not to be written back, yet. In the typical 4k
blocksize case it's only one buffer that needs to be maintained, so the
cost doesnt look like to be big. There are probably other issues to be
solved as well, but this would be the main approach. 
can you (or anyone else) see any problems with this approach? 
-- mingo
--- linux/fs/buffer.c.orig3	Fri Dec 10 05:43:59 1999
+++ linux/fs/buffer.c	Fri Dec 10 05:54:58 1999
@@ -531,6 +531,8 @@
 {
 	struct buffer_head **head = &hash(bh->b_dev, bh->b_blocknr);
 
+	if (!buffer_mapped(bh))
+		BUG();
 	spin_lock(&lru_list_lock);
 	write_lock(&hash_table_lock);
 	__hash_link(bh, head);
@@ -1208,8 +1210,8 @@
 		init_buffer(bh, end_buffer_io_async, NULL);
 		bh->b_dev = dev;
 		bh->b_blocknr = block;
-
 		set_bit(BH_Mapped, &bh->b_state);
+		insert_into_queues(bh);
 	}
 	tail->b_this_page = head;
 	get_page(page);
@@ -1868,6 +1870,8 @@
 
 		init_buffer(bh, end_buffer_io_async, NULL);
 		atomic_inc(&bh->b_count);
+		if (buffer_mapped(bh))
+			insert_into_queues(bh);
 		arr[nr] = bh;
 		nr++;
 	} while (i++, iblock++, (bh = bh->b_this_page) != head);