Date: 10 Sep 2004 17:34:44 -0700
From: Mingming Cao <>
Subject: Re: [Patch 0/6]: Cleanup and rbtree for ext3 reservations in 2.6.9-rc1-mm4
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2004/9/10/323

On Tue, 2004-09-07 at 06:02, Stephen Tweedie wrote:
> The patches in the following set contain several cleanups for ext3
> reservations, fix a reproducable SMP race, and turn the per-superblock
> linear list of reservations into an rbtree for better scaling.
> These changes have been in rawhide for a couple of weeks, and have
> been undergoing testing both within Red Hat and at IBM.  
> 
We have run several tests on this set of the reservation changes. We
compared the results w/o reservation, rbtree based reservation vs link
list based reservation. Here is the tiobench sequential test
results.Note that 2.6.8.1-mm4 kernel include the double link based
per-fs reservation tree.
            tiobench sequential write throughputs
============================================================
Threads no reservation  2.6.8.1-mm4     2.6.8.1-mm4+rbtree patch
1       29              29              29
4       3               29              29
8       4               28              28
16      3               27              27
32      4               27              27
64      3               27              27
128     2               20              25
256     1               20              24 
We did see the rbtree changes scales better on more than 128 threads. We
also run tio random tests, did not see throughput regression there. 
We also re-run the dbench, test results showing that these two
reservations(rbtree based vs link based) performs almost equally well.
dbench average throughputs on 4 runs
==================================================
Threads no reservation  2.6.8.1-mm4     2.6.8.1-mm4_rbtree
1       97              93              96
4       234             250             213
8       201             213             213
16      156             168             169
32      73              106             105
64      38              65              67
We had some concerns about the cpu cost for seeky random write workload
with all the reservation changes before. We are doing some tests in that
area too. 
Mingming
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  
http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  
http://www.tux.org/lkml/