Date: Thu, 25 Nov 1999 17:16:36 +0100 (CET)
From: Andrea Arcangeli <>
Subject: Re: [patch] smp-2.3.30-A1, mb(), wmb(), rmb()
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/1999/11/25/72

On Thu, 25 Nov 1999, Ingo Molnar wrote:
>#define mb()    __asm__ __volatile__ ("addl $0, (%%esp)": : :"memory")
>#define rmb()   mb()
>#define wmb()   __asm__ __volatile__ ("": : :"memory")
>#define set_rmb(var, value) do { var = value; rmb(); } while (0)
>#define set_mb(var, value) set_rmb(var, value)
>#define set_wmb(var, value) do { var = value; wmb(); } while (0)
Your "addl $0, (%%esp)" make no sense to me.
This is what I understood from this interesting thread. Patch (untested)
against 2.3.29:
--- 2.3.29-IA32/arch/i386/mm/ioremap.c.~1~	Wed Nov 24 18:22:04 1999
+++ 2.3.29-IA32/arch/i386/mm/ioremap.c	Thu Nov 25 17:07:21 1999
@@ -29,7 +29,7 @@
 			BUG();
 		}
 		set_pte(pte, mk_pte_phys(phys_addr, __pgprot(_PAGE_PRESENT | _PAGE_RW | 
-					_PAGE_DIRTY | _PAGE_ACCESSED | flags)));
+					_PAGE_DIRTY | _PAGE_ACCESSED | _PAGE_PCD | flags)));
 		address += PAGE_SIZE;
 		phys_addr += PAGE_SIZE;
 		pte++;
--- 2.3.29-IA32/include/asm-i386/system.h.~1~	Tue Nov 23 15:29:21 1999
+++ 2.3.29-IA32/include/asm-i386/system.h	Thu Nov 25 17:07:00 1999
@@ -158,23 +158,25 @@
 	return x;
 }
 
-/*
- * Force strict CPU ordering.
- * And yes, this is required on UP too when we're talking
- * to devices.
- *
- * For now, "wmb()" doesn't actually do anything, as all
- * Intel CPU's follow what Intel calls a *Processor Order*,
- * in which all writes are seen in the program order even
- * outside the CPU.
- *
- * I expect future Intel CPU's to have a weaker ordering,
- * but I'd also expect them to finally get their act together
- * and add some real memory barriers if so.
- */
-#define mb() 	__asm__ __volatile__ ("lock; addl $0,0(%%esp)": : :"memory")
-#define rmb()	mb()
-#define wmb()	__asm__ __volatile__ ("": : :"memory")
+/* IA32 UP enforce strong ordering.
+
+   IA32 SMP enforce Processor Ordering on Write Back memory.
+
+	Stores from one processor may not all be seen by other processors
+	yet, though if any one store is observed by processor A on
+	processor B, any earlier stores (in program order) from processor
+	A are guaranteed to be observed by processor B.
+
+   So any kind of SMP barrier is not necessary. We only must generate
+   the write assembler code, thus only the compiler barrier is necessary.
+
+   While talking with device driver we must simply make sure
+   to map the device driver memory in non cacheable memory
+   and ioremap will handle that (it's necessary for UP too). */
+
+#define mb()	barrier()
+#define rmb()	barrier()
+#define wmb()	barrier()
 #define set_rmb(var, value) do { xchg(&var, value); } while (0)
 #define set_mb(var, value) set_rmb(var, value)
 #define set_wmb(var, value) do { var = value; wmb(); } while (0)
--- 2.3.29-IA32/include/asm-i386/spinlock.h.~1~	Fri Oct  8 15:16:17 1999
+++ 2.3.29-IA32/include/asm-i386/spinlock.h	Thu Nov 25 17:12:26 1999
@@ -36,7 +36,7 @@
 	".previous"
 
 #define spin_unlock_string \
-	"lock ; btrl $0,%0"
+	"movl $0, %0"
 
 #define spin_lock(lock) \
 __asm__ __volatile__( \
Andrea
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.rutgers.edu
Please read the FAQ at 
http://www.tux.org/lkml/