Date: 29 Feb 2000 09:16:52 -0500
From:  nbecker@fred ...
Subject: Re: very large directories
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2000/2/29/102

>>>>> "Andrew" == Andrew McNabb <amcnabb@argus-systems.com> writes:
    Andrew> Disk operations are always slower if the disk is busy to begin with.
    Andrew> On 29 Feb 2000 nbecker@fred.net wrote:
    >> A coworker has an application that produces 10's of thousands of small
    >> files.  It seems the performance of even a simple operation, such as
    >> 'rm -rf' or find | xargs rm is very very slow.
    >> 
    >> What are the mechanisms that limit performance?  Is it the type of
    >> data structure used to represent directories?
 
Sorry, I wasn't very clear.  I mean that after the program is done
creating all these files, that any operations on the files are
extremely slow.  At this time the disk activity has stopped.
I suspect the answer is that the data structures used to represent
directories are not well suited to efficient operations on extremely
large directories, but I was just curious.
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.rutgers.edu
Please read the FAQ at 
http://www.tux.org/lkml/