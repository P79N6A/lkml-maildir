Date: Sun, 6 Jan 2002 23:27:39 -0200
From: Arnaldo Carvalho de Melo <>
Subject: Re: [RFC] [PATCH] Clean up fs.h union for ext2
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2002/1/6/139

Em Mon, Jan 07, 2002 at 12:30:58AM +0000, Anton Altaparmakov escreveu:
> At 22:42 06/01/02, Daniel Phillips wrote:
> >I wrote:
> >> To be honest I fail to see how one additional slab allocation will make
> >> any difference.                                                      /
> >                                                                      /
> >You could say the same about any aspect of Linux: and, relaxing your /
> >standards in such a way, you would inevitably end up with a dog.  A /
> >good fast system emerges from its many small perfections.  Half of /
> >the number of cache entries for inodes qualifies as one of those. /
> 
> Due to the nature of the content in the vfs vs. fs inode I would expect 
> that one is used independent of the other in many, if not in the majority 
> of cases. If this is correct, then it might well be an actual benefit to 
> have the two separate and to benefit from the hwcache line alignment in the 
> fs specific part. Also considering that allocation happens once in 
> read_inode but the structure is then accessed many times.
> 
> Please note, I am not saying you are wrong, most likely you are quite right 
> in fact, I am just raising a caution flag that perhaps benchmarks of both 
> implementations for the same fs might be a Good Idea(TM)...
Yes, having benchmarks done is a good idea and can clear any doubts about
the validity of such approach on a performace point of view.
When I did similar work for the network protocols, cleaning up
include/net/fs.h DaveM asked for benchmarks to see if the new approach,
i.e., using per network family slabcaches would lead to a performance drop,
I did it and we realized that it lead to performance _gains_, that in turn
made DaveM ask for a per network protocol slabcache, which made furter
memory savings and lead to further performance gains.
Yes, the usage pattern for sockets and inodes is different, thats why having
Daniel patches benchmarked against the current scheme can clear up the
matter about the validity of having the slabcaches.
Please note that we can have both approaches by leaving the
generic_ip/generic_sbp. In the struct sock case I left protinfo as a void
pointer, like generic_ip and some protocols use the slabcache approach
while others use the private area allocated separately and its pointer
stored in struct sock->protinfo.
- Arnaldo
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  
http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  
http://www.tux.org/lkml/