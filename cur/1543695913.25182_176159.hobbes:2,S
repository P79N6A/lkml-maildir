Date: Sun, 2 Nov 2003 11:04:16 +0000
From: "Dr. David Alan Gilbert" <>
Subject: Re: many ide drives, raid0/raid5
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2003/11/2/28

* Florian Reitmeir (squat@riot.org) wrote:
> Hi,
> 
> i'm using on the machine kernel 2.6.0-test6-mm1 and 15 IDE
> Drives. Everything worked fine (uptime about 30 days) > I used, some
> raid5's, below some raid0's and on top evms.
> 
> heres "cat /proc/mdstat" so its more clear
> 
> ============ cut 
> Personalities : [raid0] [raid5]
> md4 : active raid5 hdp[2] hdo[1] hdn[0]
>       120627072 blocks level 5, 32k chunk, algorithm 2 [4/3] [UUU_]
This isn't too unusual; if as you say the drives are fine then I'd agree
it is probably a controller issue.  I've got a RAID that does this
regularly and you normally find in the log somewhere an IDE error of
some type (typically it dropping out of DMA due to a busy or it not
responding).
I've given up using multiple IDE PCI cards for this - they only just
work, and occasionally you'll get a glitch like this.  I've tried
a mixture of Promise and HPT cards.  In the end I gave up and got
a 3ware IDE RAID card which is working fine (you could just use it
as a large multichannel IDE card and run soft raid if you want I think).
Dave
 -----Open up your eyes, open up your mind, open up your code -------   
/ Dr. David Alan Gilbert    | Running GNU/Linux on Alpha,68K| Happy  \ 
\ gro.gilbert @ treblig.org | MIPS,x86,ARM,SPARC,PPC & HPPA | In Hex /
 \ _________________________|_____ 
http://www.treblig.org
   |_______/
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  
http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  
http://www.tux.org/lkml/