Date: Fri, 12 Nov 1999 12:16:15 +0100
From: Jes Sorensen <>
Subject: xchg() vs test_and_set_bit()
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/1999/11/12/31

Hi
A lot of drivers use {test_and_set,test,clear}_bit() to control access
to critical code sequences since these operations are guaranteed to be
atomic, ie. dev->tbusy is a classic example in the network drivers.
However, one could achieve the same effect by using xchg(), and looking
at the code sequences on the x86 and Alpha it actually looks shorter. Of
course I asume the atomicity of the instruction is the big issue, but
why not save a word in an instruction cache line if one can? ;-)
Besides, I can imagine that bitops are likely to be much more complex on
some architectures, there must be some that don't have bitops operating
directly on memory.
Jes
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.rutgers.edu
Please read the FAQ at 
http://www.tux.org/lkml/