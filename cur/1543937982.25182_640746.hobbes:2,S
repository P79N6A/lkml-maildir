Date: Sat, 19 Jan 2008 12:17:01 +0100 (MET)
From: Andrea Righi <>
Subject: Re: [PATCH] cgroup: limit block I/O bandwidth
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2008/1/19/45

Naveen Gupta wrote:
>> Paul Menage wrote:
>>> On Jan 18,  2008 7:36 AM, Dhaval Giani <dhaval@linux.vnet.ibm.com>  wrote:
>>>> On Fri, Jan 18, 2008 at 12:41:03PM +0100, Andrea Righi  wrote:
>>>>> Allow to limit the  block I/O bandwidth for  specific process containers
>>>>> (cgroups) imposing additional delays  on I/O requests for those processes
>>>>> that exceed the  limits defined in the control group filesystem.
>>>>>
>>>>>  Example:
>>>>>   # mkdir /dev/cgroup
>>>>>   # mount -t cgroup -oio-throttle io-throttle /dev/cgroup
>>>> Just a minor nit, can't we name it as io,  keeping in mind that other
>>>> controllers are known as cpu and  memory?
>>> Or maybe "blockio"?
>> Agree, blockio seems better. Not all I/O is performed on  block devices
>> and in this case we're  considering block devices only.
> 
> Here we want to rate limit in block layer, I would think I/O scheduler
> is the place where we are in much better position to do this kind of
> limiting.
> 
> Also we are changing the behavior of application by adding sleeps to
> it during request submission. Moreover, we will prevent requests from
> being merged since we won't allow them to be submitted in this case.
> 
> Since bulk of submission for writes is done in background kernel
> threads and we throttle based on limits on current, we will end up
> throttling these threads and not the actual processes submitting i/o.
Yep, that's true! This works for read operations only... at the very
least, if I've understood well, we could throttle I/O reads in the
submit_bio() path and write operations in __set_page_dirty(). But this
would change the applications behavior, so probably the best approcah
could be to just get I/O statistics from TASK_IO_ACCOUNTING stuff and
implement task delays at the I/O scheduler layer...
Thanks,
-Andrea