Date: Fri, 19 May 2006 14:00:21 -0400
From: Chuck Lever <>
Subject: [PATCH 1/6] nfs: "open code" the NFS direct write rescheduler
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2006/5/19/140

In order to support direct asynchronous vectored writes, the write
rescheduler in fs/nfs/direct.c must not depend on the I/O parameters in the
controlling nfs_direct_req structure.  Refactor the write rescheduler to
use only the information in each nfs_write_data structure to redrive writes
if the server rebooted before we can commit them.
Signed-off-by: Chuck Lever <cel@netapp.com>
---
 fs/nfs/direct.c |   71 +++++++++++++++++++++++++++++++++++++++++++------------
 1 files changed, 56 insertions(+), 15 deletions(-)
diff --git a/fs/nfs/direct.c b/fs/nfs/direct.c
index 0980e43..0fde4bf 100644
--- a/fs/nfs/direct.c
+++ b/fs/nfs/direct.c
@@ -94,8 +94,19 @@ struct nfs_direct_req {
 	struct nfs_writeverf	verf;		/* unstable write verifier */
 };
 
-static void nfs_direct_write_schedule(struct nfs_direct_req *dreq, int sync);
 static void nfs_direct_write_complete(struct nfs_direct_req *dreq, struct inode *inode);
+static const struct rpc_call_ops nfs_write_direct_ops;
+
+static inline int nfs_direct_dec_outstanding(struct nfs_direct_req *dreq)
+{
+	int result;
+
+	spin_lock(&dreq->lock);
+	result = --dreq->outstanding;
+	spin_unlock(&dreq->lock);
+
+	return (result == 0);
+}
 
 /**
  * nfs_direct_IO - NFS address space operation for direct I/O
@@ -428,14 +439,52 @@ static void nfs_direct_free_writedata(st
 #if defined(CONFIG_NFS_V3) || defined(CONFIG_NFS_V4)
 static void nfs_direct_write_reschedule(struct nfs_direct_req *dreq)
 {
+	struct inode *inode = dreq->inode;
 	struct list_head *pos;
 
-	list_splice_init(&dreq->rewrite_list, &dreq->list);
-	list_for_each(pos, &dreq->list)
-		dreq->outstanding++;
+	/*
+	 * Prevent I/O completion while we're still rescheduling
+	 */
+	dreq->outstanding++;
+
 	dreq->count = 0;
+	list_for_each(pos, &dreq->rewrite_list) {
+		struct nfs_write_data *data =
+			list_entry(dreq->rewrite_list.next, struct nfs_write_data, pages);
+
+		spin_lock(&dreq->lock);
+		dreq->outstanding++;
+		spin_unlock(&dreq->lock);
+
+		nfs_fattr_init(&data->fattr);
+		data->res.count = data->args.count;
+		memset(&data->verf, 0, sizeof(data->verf));
+
+		rpc_init_task(&data->task, NFS_CLIENT(inode), RPC_TASK_ASYNC,
+				&nfs_write_direct_ops, data);
+		NFS_PROTO(inode)->write_setup(data, FLUSH_STABLE);
+
+		data->task.tk_priority = RPC_PRIORITY_HIGH;
+		data->task.tk_cookie = (unsigned long) inode;
 
-	nfs_direct_write_schedule(dreq, FLUSH_STABLE);
+		lock_kernel();
+		rpc_execute(&data->task);
+		unlock_kernel();
+
+		dfprintk(VFS, "NFS: %5u rescheduled direct write call (req %s/%Ld, %zu bytes @ offset %Lu)\n",
+				data->task.tk_pid,
+				inode->i_sb->s_id,
+				(long long)NFS_FILEID(inode),
+				data->args.count,
+				(unsigned long long)data->args.offset);
+	}
+
+	/*
+	 * If we raced with the rescheduled I/O and lost, then
+	 * all the I/O is already complete.
+	 */
+	if (nfs_direct_dec_outstanding(dreq))
+		nfs_direct_write_complete(dreq, inode);
 }
 
 static void nfs_direct_commit_result(struct rpc_task *task, void *calldata)
@@ -605,8 +654,6 @@ static void nfs_direct_write_result(stru
 				}
 		}
 	}
-	/* In case we have to resend */
-	data->args.stable = NFS_FILE_SYNC;
 
 	spin_unlock(&dreq->lock);
 }
@@ -620,14 +667,8 @@ static void nfs_direct_write_release(voi
 	struct nfs_write_data *data = calldata;
 	struct nfs_direct_req *dreq = (struct nfs_direct_req *) data->req;
 
-	spin_lock(&dreq->lock);
-	if (--dreq->outstanding) {
-		spin_unlock(&dreq->lock);
-		return;
-	}
-	spin_unlock(&dreq->lock);
-
-	nfs_direct_write_complete(dreq, data->inode);
+	if (nfs_direct_dec_outstanding(dreq))
+		nfs_direct_write_complete(dreq, data->inode);
 }
 
 static const struct rpc_call_ops nfs_write_direct_ops = {
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  
http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  
http://www.tux.org/lkml/