Date: Fri, 01 Nov 2002 19:35:36 -0800
From: Andrew Morton <>
Subject: Re: idle time & iowait accounting
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2002/11/1/312

William Lee Irwin III wrote:
> 
> ...
> 
> +unsigned long nr_iowait(void)
> +{
> +       unsigned long i, sum = 0;
> +
> +       for (i = 0; i < NR_CPUS; ++i)
> +               sum += atomic_read(&cpu_rq(i)->nr_iowait);
> +
> +       return sum;
> +}
> +
We need to make a habit of checking cpu_online(i) in here.  I'd rather
this consume 4 cachelines than 32, thanks ;)
Also, if we decide to allocate each CPU's per-cpu memory separately
any for-all-CPUs loop which uses per-cpu data MUST make this check,
else it'll oops.
Not applicable in this case, and alas the cpu iterator helper macros
(for_each_online_cpu(), etc) are currently AWOL, but...
> +void io_schedule(void)
> +{
> +       struct runqueue *rq;
> +       preempt_disable();
> +       rq = this_rq();
> +       atomic_inc(&rq->nr_iowait);
> +       schedule();
> +       atomic_dec(&rq->nr_iowait);
> +       preempt_enable();
> +}
"scheduling while atomic".  You'll need to reacquire the runqueue pointer
on waking up.
> +
> +void io_schedule_timeout(long timeout)
> +{
> +       struct runqueue *rq;
> +       preempt_disable();
> +       rq = this_rq();
> +       atomic_inc(&rq->nr_iowait);
> +       schedule_timeout(timeout);
> +       atomic_dec(&rq->nr_iowait);
> +       preempt_enable();
> +}
And here too.
Apart from that, looks very good, thanks.
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  
http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  
http://www.tux.org/lkml/