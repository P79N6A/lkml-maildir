Date: Thu, 14 Jan 1999 15:26:03 -0500
From: "Fred Reimer" <>
Subject: RE: [patch] arca-vm-19 [Re: Results: Zlatko's new vm patch]
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/1999/1/14/196

It would not be possible to implement for 2.2.0, because it's frozen except
bugs right?  But, would it be possible/desireable to have different
algorithms selectable as part of the kernel build process?  Are their
different algorithms that would be useful for different types of systems and
possibly not one "best" algorithm?  If that's the case then it would make
sense, IMHO, to allow for selection in the kernel configuration process,
defaulting to the "best" algorithm for general use.
Fred
> -----Original Message-----
> From: owner-linux-kernel@vger.rutgers.edu
> [mailto:owner-linux-kernel@vger.rutgers.edu]On Behalf Of Heinz
> Mauelshagen
> Sent: Thursday, January 14, 1999 1:02 PM
> To: Andrea Arcangeli
> Cc: mge@ts1.ez-darmstadt.telekom.de; linux-kernel@vger.rutgers.edu
> Subject: Re: [patch] arca-vm-19 [Re: Results: Zlatko's new vm patch]
>
>
> >
> > On Wed, 13 Jan 1999, Andrea Arcangeli wrote:
> >
> > > I produced a new arca-vm-19. I would like if you could
> try it. I don't
> >
> > It seems that the better algorithm I am be able to invent
> is been the
> > growing_swap_cache one (the one in arca-vm-16). Steve could
> you try this
> > new patch (arca-vm-20) against real 2.2.0-pre7? I think
> that it should be
> > still better than arca-vm-16 + SWAP_CLUSTER_MAX=512.
> >
> > If it will be not very good could you do:
> >
> > echo 8 2 4 512 512 512 > /proc/sys/vm/pager
> >
> > and try again? (such numbers should be the same of setting
> > SWAP_CLUSTER_MAX in arca-vm-16, but as default only the
> max_async_pages is
> > set to 512 because I think it's been the only one that made
> a difference).
> >
> > If this will be not the best again you could apply the
> filemap.c patch I
> > sent you in the last email (the one that return to put the
> shrink_mmap()
> > weight exponential increasing in function of priority) and
> try again?
> >
>
> Hi Andrea!
>
> I tested vm20 against bare 2.2.0-pre7 and it doesn't do the job 8*(
>
>
> The system is a little bit more responsive while doing a 12G mke2fs
> now only taking about 10-20 seconds ;-( to bring up another kvt
> compared to end of mke2fs without vm20.
>
> But compared to < 2.2.0 VM this is about at least 10-20!!!
> times slower.
>
>
> The system (2*PII/350, 256M RAM) swaps out 30MB (kvts etc.) for _no_
> obvious reason with paging rates up to > 100/s ?!
>
> I don't like 1 process eating up main memory for buffer/page cache
> _and_ thereby causing swaping out inactive others.
>
> Other testers reported the same effect, if the system runs
> for some days under medium load.
>
> /proc,sys/vm/{buffermemm, pagecache} are gone now because
> they haven't been
> used any longer, but at least i liked them.
>
> Why not keep that interface and the thereby implied limits in
> the kernel
> VM to allow the admin to set them to 100% if he likes to?
>
> Sorry if this sounds too bad, but to my opinion this
> behaviour is an absolute
> _must_ change before the final 2.2 release!
>
> Keep up the good work,
> Heinz
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.rutgers.edu
Please read the FAQ at 
http://www.tux.org/lkml/