Date: Mon, 1 Oct 2007 18:30:59 -0400
From: William Cattey <>
Subject: Re: vm86.c audit_syscall_exit() call trashes registers
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2007/10/1/343

Thanks very much for responding.
 From your two replies, I crafted the attached patch.
Alas, the EDID transfer comes up all zeros.
I see two possible causes of this behavior:
1. I misunderstood how you intended the file to be modified.
2. The fix for my bug is NOT in correcting the audit call, but  
instead from some other fix, perhaps from the other aspect that you  
worked on.
I know that when I surrounded the audit_syscall_exit with #if 0 I got  
correct EDID fetches, so the most likely cause is #1, that I didn't  
correctly incorporate your understanding of correct operation of the  
call to audit_syscall_exit.
Here's my patch.  Where did I screw up?
--- linux-2.6.18.i686/arch/i386/kernel/vm86.c.foonly	2007-10-01 16:40:35.000000000 -0400
+++ linux-2.6.18.i686/arch/i386/kernel/vm86.c	2007-10-01 16:27:50.000000000 -0400
@@ -318,12 +318,11 @@
 	tsk->thread.screen_bitmap = info->screen_bitmap;
 	if (info->flags & VM86_SCREEN_BITMAP)
 		mark_screen_rdonly(tsk->mm);
-	__asm__ __volatile__("xorl %eax,%eax; movl %eax,%fs; movl %eax,%gs\n\t");
-	__asm__ __volatile__("movl %%eax, %0\n" :"=r"(eax));
+	__asm__ __volatile__("mov %0, %%fs; mov %0, %%gs" : : "r" (0));
 
 	/*call audit_syscall_exit since we do not exit via the normal paths */
 	if (unlikely(current->audit_context))
-		audit_syscall_exit(AUDITSC_RESULT(eax), eax);
+		audit_syscall_exit(AUDITSC_RESULT(0), 0);
 
 	__asm__ __volatile__(
 		"movl %0,%%esp\n\t"
-Bill
----
William Cattey
Linux Platform Coordinator
MIT Information Services & Technology
N42-040M, 617-253-0140, wdc@mit.edu
http://web.mit.edu/wdc/www/
On Sep 29, 2007, at 2:09 AM, Jeremy Fitzhardinge wrote:
> Jeremy Fitzhardinge wrote:
>> @@ -306,19 +334,18 @@ static void do_sys_vm86(struct kernel_vm
>>         tsk->thread.screen_bitmap = info->screen_bitmap;
>>         if (info->flags & VM86_SCREEN_BITMAP)
>>                 mark_screen_rdonly(tsk->mm);
>>         __asm__ __volatile__("xorl %eax,%eax; movl %eax,%fs; movl % 
>> eax,%gs\n\t");
>>
>
> Oh, this line is also clearly bogus, since it clobbers %eax without
> telling the compiler.  The minimal change would be something like:
>
> 	asm volatile("mov %0, %%fs; mov %0, %%gs" : : "r" (0));
>
>
>     J