Date: Fri, 26 Nov 1999 03:04:29 +0100 (CET)
From: Ingo Molnar <>
Subject: Re: spin_unlock optimization(i386)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/1999/11/25/167

> 	for (;;) {			for (;;) {
> 		i++;				a = i;
> 		j++;				b = j;
> 	}					if (a < b)
> 							BUG()
> 					}
in fact with the attached testcode causality seems to break _easily_ on
Xeon CPUs:
	moon:~> ./causal 2
	<30> 9 11
	<thread1> BROKE causality! Weakly ordered memory?
the code starts two threads which do the above test. Ie. we can _easily_
see stores on other CPUs in the wrong order.
So you and Manfred are right :) This means that the barrier changes in
smp-2.3.30-A1 still hold, plus i believe all ioremapped area has to be
made uncacheable on x86.
the barrier changes in smp-2.3.30-A1 are done only based on the rule that
writes never pass reads upwards. (reads can still pass reads upwards, and
reads can still pass writes upwards)
	Ingo
/*
 * Linux SMP memory model and SMP causality tester, Ingo Molnar
 *
 * Copyright (C) 1999, Ingo Molnar <mingo@redhat.com>
 */
#include <unistd.h>
#include <stdio.h>
#include <stdlib.h>
#include <signal.h>
#include <sys/wait.h>
#include <linux/unistd.h>
volatile static int started = 0;
static int numthreads;
#define mb()    __asm__ __volatile__ ("lock; addl $0,0(%%esp)": : :"memory")
volatile int data1 = 0, filler03[8], data2 = 0;
static int test_locking(int cpu)
{
	int a, b, count = 0;
	if (cpu < 1) {
		for (;;) {
			data1++; 
			data2++;
		}
	} else {
		for (;;) {
			count++;
			a = data1;
			b = data2;
			if (a < b) {
				printf("<%d> %d %d\n", count, a, b);
				return 1;
			}
		}
	}
}
void test_causality (int cpu)
{
	asm volatile ("lock; incl %0":"=m"(started));
	while (numthreads != started) mb();
	test_locking(cpu);
	printf("<thread%d> BROKE causality! Weakly ordered memory?\n", cpu);
	exit(0);
}
static void start_thread(int cpu)
{
	char *newstack = (char *) malloc(10000) + 5000;
	*newstack = cpu;
	__asm__ __volatile__(
		"int $0x80	\n\t"	/* Linux/i386 system call */
		"testl %0,%0	\n\t"	/* check return value */
		"jne 1f		\n\t"	/* jump if parent */
		"call *%2	\n\t"	/* start subthread function */
		"movl %1,%0	\n\t"
		"int $0x80	\n\t"	/* exit system call: exit subthread */
		"1:		\n\t"
		: :"a" (__NR_clone),"i" (__NR_exit), "r" (test_causality),
		   "b" (0xaf00 | SIGCHLD), "c" (newstack));
	return;
}
int main (int argc, char * * argv)
{
	int i;
	if (argc != 2) {
		printf("usage: causal <kids>\n");
		exit(0);
	}
	numthreads = atol(argv[1]);
	for (i = 0; i<numthreads; i++) {
		start_thread(i);
	}
	return (0);
}
--- linux/include/asm-i386/system.h.orig	Fri Oct 15 18:29:42 1999
+++ linux/include/asm-i386/system.h	Thu Nov 25 02:34:57 1999
@@ -161,21 +161,31 @@
 /*
  * Force strict CPU ordering.
  * And yes, this is required on UP too when we're talking
- * to devices.
+ * to devices. Note that these primitives deal with operation
+ * ordering, not atomicity.
  *
  * For now, "wmb()" doesn't actually do anything, as all
  * Intel CPU's follow what Intel calls a *Processor Order*,
  * in which all writes are seen in the program order even
  * outside the CPU.
  *
- * I expect future Intel CPU's to have a weaker ordering,
- * but I'd also expect them to finally get their act together
- * and add some real memory barriers if so.
+ * rmb(): writes are guaranteed by Intel to never pass reads
+ * (in an externally visible way that is), so the read barrier
+ * is just a dummy store. Note that (%%esp) is ours and only
+ * ours, so this is IRQ-safe as well. The barrier _must_
+ * include a store, thus the assembly. The CPU snoops external
+ * devices (including CPUs and DMA agents), so this works out
+ * just fine in all cases and is lightweight. And this is only
+ * 4 bytes icache footprint total. 'addl $0, (%%esp)' is a legal
+ * write operation which dirties the target cacheline.
+ *
+ * We expect future Intel CPU's to have a weaker ordering. But
+ * it's not all that bad right now already.
  */
-#define mb() 	__asm__ __volatile__ ("lock; addl $0,0(%%esp)": : :"memory")
+#define mb() 	__asm__ __volatile__ ("addl $0, (%%esp)": : :"memory")
 #define rmb()	mb()
 #define wmb()	__asm__ __volatile__ ("": : :"memory")
-#define set_rmb(var, value) do { xchg(&var, value); } while (0)
+#define set_rmb(var, value) do { var = value; rmb(); } while (0)
 #define set_mb(var, value) set_rmb(var, value)
 #define set_wmb(var, value) do { var = value; wmb(); } while (0)
 