Date: Thu, 10 Aug 2000 07:30:33 -0700
From: David Lombard <>
Subject: Re: O_LARGEFILE
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2000/8/10/205

Jesse Pollard wrote:
> 
> Russell Coker <russell@coker.com.au>:
> > On Solaris when I open a device file for a >2G hard drive without O_LARGEFILE
> > then I can only read the first 2G of data.  On Linux 2.4.0-test5 (and all
> > other versions I have tested) and Glibc 2.1.3 (and all other libc versions I
> > have tested) I can open /dev/hda without O_LARGEFILE and read as much data as
> > there is.
> > Does this constitute a bug in Linux?  Is it planned to be fixed?
> >
> > I know this seems a bit picky, but it would be easier to develop software
> > that is portable to other versions of Unix if this sort of thing just worked.
> 
> Ummm ... I don't see it as a bug, but a feature. This is the same action
> that occurs on most systems that natively support >2G files. The O_LARGEFILE
> is a no-op, since it works the same either way. (Cray UNICOS/SGI/and I believe
> the IBM SP series).
You opened a block-special file, and that isn't protected by the LFS. 
One reason for this "exemption" was that at the time of the LFS, some
existing programs could already read up to 4GB on some 32-bit systems if
they read a block-special or character-special file.
So, both Solaris and Linux operate correctly.  Unfortunately for you,
they also operate differently.
If you had read a regular file past 2GB w/o O_LARGEFILE or
_LARGEFILE_SOURCE, then you would have hit an bug.
BTW, the LFS never applied to UNICOS (ILP-64), IRIX64 (LP-64), et al.
-- 
David N. Lombard
MSC.Software
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.rutgers.edu
Please read the FAQ at 
http://www.tux.org/lkml/