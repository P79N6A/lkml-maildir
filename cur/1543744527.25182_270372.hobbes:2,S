Date: Thu, 11 Nov 2004 16:32:45 -0600
From: Terence Ripperda <>
Subject: [patch] VM accounting change
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2004/11/11/204

I apologize for not having more first hand info on this problem.
I've been told that recent changes in vm accounting in mmap have
caused some problems with our driver during mmap.
the problem seems to stem from us oring vma->vm_flags w/ (VM_LOCKED 
| VM_IO). we do this for agp and pci pages that are mapped to push
buffers. VM_LOCKED since we don't want the pages to move and VM_IO so
they are not dumped during a core dump.
I've been told that the new vm accounting changes call
make_pages_present() for all pages in a vma range marked VM_LOCKED and
that this causes problems with our driver. the attached patch modifies
this to not call make_pages_present() if VM_IO is also set.
what I'm unclear on is if this is a valid fix. should
make_pages_present work for memory ranges that are already mapped and
locked down (via PG_reserved)? are we wrong in using (VM_IO |
VM_LOCKED) for our pages? is this fix correct?
Thanks,
Terence
diff -ru linux-2.6.10-rc1-bk8/mm/mmap.c linux-2.6.10-rc1-bk8-2/mm/mmap.c
--- linux-2.6.10-rc1-bk8/mm/mmap.c	2004-11-06 15:04:28.000000000 +0100
+++ linux-2.6.10-rc1-bk8-2/mm/mmap.c	2004-11-06 15:39:47.000000000 +0100
@@ -1011,7 +1011,8 @@
 	__vm_stat_account(mm, vm_flags, file, len >> PAGE_SHIFT);
 	if (vm_flags & VM_LOCKED) {
 		mm->locked_vm += len >> PAGE_SHIFT;
-		make_pages_present(addr, addr + len);
+		if (!(vm_flags & VM_IO))
+			make_pages_present(addr, addr + len);
 	}
 	if (flags & MAP_POPULATE) {
 		up_write(&mm->mmap_sem);