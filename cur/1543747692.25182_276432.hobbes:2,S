Date: Fri, 3 Dec 2004 10:39:03 +0100
From: Jens Axboe <>
Subject: Re: Time sliced CFQ io scheduler
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2004/12/3/33

On Fri, Dec 03 2004, Andrew Morton wrote:
> "Prakash K. Cheemplavam" <prakashkc@gmx.de> wrote:
> >
> > > Can you try with the patch that is in the parent of this thread? The
> >  > above doesn't look that bad, although read performance could be better
> >  > of course. But try with the patch please, I'm sure it should help you
> >  > quite a lot.
> >  > 
> > 
> >  It actually got worse: Though the read rate seems accepteble, it is not, as 
> >  interactivity is dead while writing.
> 
> Is this a parallel IDE system?  SATA?  SCSI?  If the latter, what driver
> and what is the TCQ depth?
Yeah, that would be interesting to know. Or of the device is on dm or
raid. And what filesystem is being used?
TCQ depth doesn't matter with cfq really, as you can fully control how
big you go with the drive (default is 4) with max_depth.
Running buffer reads and writes here with new cfq, I get about ~7MiB/sec
read and ~14MiB/sec write aggregate performance with 4 clients (2 of
each) with the default settings. If I up idle period to 6ms and slice
period to 150ms, I get ~13MiB/sec read and ~11MiB/sec write aggregate
for the same run.
So Prakash, please try the same test with those settings:
# cd /sys/block/<dev>/queue/iosched
# echo 6 > idle
# echo 150 > slice
These are the first I tried, there may be better settings. If you have
your filesystem on dm/raid, you probably want to do the above for each
device the dm/raid is composed of.
-- 
Jens Axboe
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  
http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  
http://www.tux.org/lkml/