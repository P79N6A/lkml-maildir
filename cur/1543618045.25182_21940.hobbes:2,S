Date: Sun, 20 Jan 2002 14:55:22 +0100 (CET)
From: Ingo Molnar <>
Subject: [sched] [patch] schedule-cleanup-2.5.3-pre2-A1
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2002/1/20/164

the attached patch speeds up and cleans up schedule(), removing the
need_reschedule check on bkl-reacquire. This was needed in 2.2 times when
the big kernel lock was still prominent, but today it's not needed
anymore, especially with the preemption/lowlatency patches that will do
this correctly. This change removes 5 lines from the source code, and a
number of instructions from the scheduler hotpath. (i've also reverted the
switch statement's indentation to the previous, more correct indentation
style.)
	Ingo
--- linux/kernel/sched.c.orig	Sun Jan 20 11:47:39 2002
+++ linux/kernel/sched.c	Sun Jan 20 11:47:53 2002
@@ -589,29 +589,26 @@
  */
 asmlinkage void schedule(void)
 {
-	task_t *prev, *next;
+	task_t *prev = current, *next;
+	runqueue_t *rq = this_rq();
 	prio_array_t *array;
-	runqueue_t *rq;
 	list_t *queue;
 	int idx;
 
 	if (unlikely(in_interrupt()))
 		BUG();
-need_resched_back:
-	prev = current;
 	release_kernel_lock(prev, smp_processor_id());
-	rq = this_rq();
 	spin_lock_irq(&rq->lock);
 
 	switch (prev->state) {
-		case TASK_INTERRUPTIBLE:
-			if (unlikely(signal_pending(prev))) {
-				prev->state = TASK_RUNNING;
-				break;
-			}
-		default:
-			deactivate_task(prev, rq);
-		case TASK_RUNNING:
+	case TASK_INTERRUPTIBLE:
+		if (unlikely(signal_pending(prev))) {
+			prev->state = TASK_RUNNING;
+			break;
+		}
+	default:
+		deactivate_task(prev, rq);
+	case TASK_RUNNING:
 	}
 pick_next_task:
 	if (unlikely(!rq->nr_running)) {
@@ -654,8 +651,6 @@
 	spin_unlock_irq(&rq->lock);
 
 	reacquire_kernel_lock(current);
-	if (need_resched())
-		goto need_resched_back;
 	return;
 }
 