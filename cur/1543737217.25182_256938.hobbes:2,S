Date: Fri, 1 Oct 2004 22:31:45 +0300
From: Denis Vlasenko <>
Subject: [PATCH] small sha256 cleanup
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2004/10/1/170

Looks like open-coded be_to_cpu.
GCC produces rather poor code for this.
be_to_cpu produces asm()s which are ~4 times shorter.
Compile-tested only.
I am not sure whether input can be 32bit-unaligned.
If it indeed can be, replace:
((u32*)(input))[I]  ->  get_unaligned( ((u32*)(input))+I )
--
vda
--- linux-2.6.9-rc3/crypto/sha256.c.org	Thu Sep 30 07:32:12 2004
+++ linux-2.6.9-rc3/crypto/sha256.c	Thu Sep 30 07:33:06 2004
@@ -63,15 +63,7 @@
 
 static inline void LOAD_OP(int I, u32 *W, const u8 *input)
 {
-	u32 t1 = input[(4 * I)] & 0xff;
-
-	t1 <<= 8;
-	t1 |= input[(4 * I) + 1] & 0xff;
-	t1 <<= 8;
-	t1 |= input[(4 * I) + 2] & 0xff;
-	t1 <<= 8;
-	t1 |= input[(4 * I) + 3] & 0xff;
-	W[I] = t1;
+	W[I] = __be32_to_cpu( ((u32*)(input))[I] );
 }
 
 static inline void BLEND_OP(int I, u32 *W)