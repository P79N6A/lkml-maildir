Date: Mon, 26 Jul 2004 14:40:59 +0200
From: Ingo Molnar <>
Subject: Re: [patch] voluntary-preempt-2.6.8-rc2-J3
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2004/7/26/58

* Lee Revell <rlrevell@joe-job.com> wrote:
> > > Yes, jackd does exactly this, mlockall then opens the ALSA driver with
> > > mmap.
> > 
> > ok, i fixed this in -J3:
> > 
> > 
http://redhat.com/~mingo/voluntary-preempt/voluntary-preempt-2.6.8-rc2-J3
> > 
> > -J3 also includes a number of softirq latency fixes for the networking
> > layer.
> 
> OK, I will try this.  I have not seen any latency issues with softirqs
> with -I4. [...]
i'm going through the subsystems systematically and i'm stressing them
beyond normal use. These networking latencies need a high number of in
flight packets, multiple TCP sockets, and a 100 mbit link or faster. 
(this is a more common workload on a corporate desktop, but not typical
on a home desktop.)
> [...]  Other than the few remaining hot spots, the only thing that
> triggers latencies over 100 usecs during normal operation is the IDE
> I/O completion, which can be easily controlled by lowering the max SG
> size.
ok, i'll take a look whether there's a way to control this without
having to artificially impact the IO patterns.
> Here is one that I think happens when deleting a large number of
> files, or a directory that had a large number of files.  Specifically,
> this happens when bonnie exits.
> 
> Jul 25 20:25:36 mindpipe kernel:
>
> [...] 16ms non-preemptible critical section violated 1 ms preempt 
> threshold starting at select_parent+0x18/0xd0 and ending at
> select_parent+0x94/0xd0
ok, this should be the dcache/icache zapping. I've done some latency
reduction in this area but apparently not enough.
> I am also seeing a lot of shorter timing violations that involve
> unmap_vmas.  Not sure what triggers this one.
it might just be a common operation, being hit by the IDE hardirq
latency. (which thus is added to the 'scheduling' latency.) Although
none of the traces show IDE hardirq leftovers [but they might be cleared
from the stack by the time we notice the latency.]
Can you see these 1-2 msec latencies even if you reduce the IDE sg-size
drastically via the max_sectors tunable? (just for testing purposes, to
eliminate hardirq latencies as much as possible)
	Ingo
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  
http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  
http://www.tux.org/lkml/