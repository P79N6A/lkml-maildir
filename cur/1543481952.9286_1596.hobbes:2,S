Date: Mon, 25 Jan 1999 02:10:37 +0000 (GMT)
From: (Alan Cox)
Subject: Re: MM deadlock [was: Re: arca-vm-8...]
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/1999/1/24/139

> If I understand well the problem is get more than 1<<maxorder contiguos
> phys pages in RAM. I think it should not too difficult to do a dirty hack
Yep. We are talking about 2->4Mb sized chunks. We are also talking about
chunks that are allocated rarely - for example when you load wave data
into the sound card, while you are capturing etc. So its blocks that
can be slow to allocate, slow to free, so long as they are normal speed
to access. That may make the problem a lot easier
> alternate __get_big_pages that does some try to get many mem-areas of the
> maximal order contigous. Maybe it will not able to give you such contiguos
> memory (due mem fragmentation) but if it's possible it will give back it
> to you (_slowly_). Then you should use an aware free_big_pages() to give
> back the memory. That way the codebase (for people that doesn't need
> __get_big_pages in their device drivers) will be untouched (so no codebase
> stability issues). 
That fact we effectively "poison" the various blocks of memory with locked
down kernel objects is what makes this so tricky. It really needs some back
pressure applied so that kernel allocations come from a limited number of
maxorder blocks, at least except under exceptional circumstances.
I think its too tricky for 2.2 even as a later retrofit
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.rutgers.edu
Please read the FAQ at 
http://www.tux.org/lkml/