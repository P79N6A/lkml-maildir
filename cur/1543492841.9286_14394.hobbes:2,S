Date: Wed, 14 Apr 1999 13:57:36 +0200 (CEST)
From: Ingo Molnar <>
Subject: Re: CPU affinity
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/1999/4/14/92

On Wed, 14 Apr 1999, Rik van Riel wrote:
> Maybe it's an idea to add this change to kernel/sched.c::goodness()
> 
>                 if (p == prev)
>                         weight += 1;
>         }
> -->
> 	else if (p->processor != this_cpu)
> 		weight = -1;
> <--
>         return weight;
> }
the problem you are trying to solve is IMO elsewhere. The above change
makes a difference only if all runnable processes have a zero-counter, ie. 
counter recalculation is due. The simple solution is to simply repeat the
goodness() run after recalculation (the relevant data structures are
cache-hot already and recalculation is a rare event nevertheless), because
otherwise the first reschedule after recalculation isnt just random. I did
this some time ago and it showed no measureable effect. (i've attached a
patch that does something like that) 
-- mingo
--- linux/kernel/sched.c.orig	Tue Mar 30 11:01:33 1999
+++ linux/kernel/sched.c	Wed Apr 14 12:56:58 1999
@@ -691,6 +691,7 @@
 
 	sched_data->prevstate = prev->state;
 
+repeat:
 /* this is the scheduler proper: */
 	{
 		struct task_struct * p = init_task.next_run;
@@ -735,6 +736,8 @@
 			for_each_task(p)
 				p->counter = (p->counter >> 1) + p->priority;
 			read_unlock(&tasklist_lock);
+			spin_lock_irq(&runqueue_lock);
+			goto repeat;
 		}
 	}
 
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.rutgers.edu
Please read the FAQ at 
http://www.tux.org/lkml/