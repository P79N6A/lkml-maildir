Date: Tue, 21 Nov 2006 16:51:22 -0800
From: "Paul E. McKenney" <>
Subject: Re: [patch] cpufreq: mark cpufreq_tsc() as core_initcall_sync
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2006/11/21/299

On Wed, Nov 22, 2006 at 12:01:05AM +0300, Oleg Nesterov wrote:
> On 11/21, Paul E. McKenney wrote:
> >
> > On Tue, Nov 21, 2006 at 12:56:21PM -0500, Alan Stern wrote:
> > > Here's another potential problem with the fast path approach.  It's not
> > > very serious, but you might want to keep it in mind.
> > >
> > > The idea is that a reader can start up on one CPU and finish on another,
> > > and a writer might see the finish event but not the start event.  For
> > > example:
> >
> > One approach to get around this would be for the the "idx" returned from
> > srcu_read_lock() to keep track of the CPU as well as the index within
> > the CPU.  This would require atomic_inc()/atomic_dec() on the fast path,
> > but would not add much to the overhead on x86 because the smp_mb() imposes
> > an atomic operation anyway.  There would be little cache thrashing in the
> > case where there is no preemption -- but if the readers almost always sleep,
> > and where it is common for the srcu_read_unlock() to run on a different CPU
> > than the srcu_read_lock(), then the additional cache thrashing could add
> > significant overhead.
> 
> If you are going to do this, it seems better to just forget about ->per_cpu_ref,
> and use only ->hardluckref[]. This also allows to avoid the polling in
> synchronize_srcu().
If the readers are reasonably rare, that could work.  If readers are
common, you get memory contention (as well as cache thrashing) on the
->hardluckref[] elements.  But putting this degree of cache thrashing
into SRCU certainly does not feel right.
						Thanx, Paul
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  
http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  
http://www.tux.org/lkml/