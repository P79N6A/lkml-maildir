Date: Thu, 28 Dec 2000 14:19:19 -0500
From: Chris Mason <>
Subject: Re: [RFC] changes to buffer.c (was Test12 ll_rw_block error)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2000/12/28/78

On Thursday, December 28, 2000 16:49:14 +0100 Daniel Phillips <phillips@innominate.de> wrote:
[ dbench 48 test on the anon space mapping patch ]
> 
> This benchmark doesn't seem to suffer a lot from noise, so the 7%
> slowdown with your patch likely real.
> 
Ok, page_launder is supposed to run through the inactive dirty
list twice, and on the second run, it wants to start i/o.  But,
if the page is dirty, writepage is called on the first run.  With
my patch, this flushes lots more data than it used to.  
I have writepage doing all the i/o, and try_to_free_buffers
only waits on it.  This diff makes it so writepage is only called
on the second loop through the inactive dirty list, could you 
please give it a try (slightly faster in my tests).
Linus and Rik are cc'd in to find out if this is a good idea in
general.
-chris
--- linux-test13-pre4/mm/vmscan.c	Sat Dec 23 13:14:26 2000
+++ linux/mm/vmscan.c	Thu Dec 28 15:02:08 2000
@@ -609,7 +609,7 @@
 				goto page_active;
 
 			/* Can't start IO? Move it to the back of the list */
-			if (!can_get_io_locks) {
+			if (!launder_loop || !can_get_io_locks) {
 				list_del(page_lru);
 				list_add(page_lru, &inactive_dirty_list);
 				UnlockPage(page);
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
Please read the FAQ at 
http://www.tux.org/lkml/