Date: Mon, 24 Oct 2005 14:05:50 -0400 (EDT)
From: Nicolas Pitre <>
Subject: [PATCH 4/5] crypto/sha1.c: avoid successively shifting a long long
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2005/10/24/138

Shifting a long long about 7 times to store the length bits is suboptimal
on most 32-bit architectures.  Use a 32 bit scratch instead.
This provides an appreciable code reduction considering the _whole_
of the sha1_final() function:
	arch		old size	new size	reduction
	---------------------------------------------------------
	i386		0xe0		0xc4		12.5%
	arm		0x15c		0xe8		33.3%
Smaller code in this case is of course faster code.
Signed-off-by: Nicolas Pitre <nico@cam.org>
Index: linux-2.6/crypto/sha1.c
===================================================================
--- linux-2.6.orig/crypto/sha1.c
+++ linux-2.6/crypto/sha1.c
@@ -75,8 +75,8 @@
 static void sha1_final(void* ctx, u8 *out)
 {
 	struct sha1_ctx *sctx = ctx;
-	u32 i, j, index, padlen;
-	u64 t, count = sctx->count;
+	u64 count = sctx->count;
+	u32 i, j, index, padlen, t;
 	u8 bits[8];
 	static const u8 padding[64] = { 0x80, };
 
@@ -90,7 +90,8 @@
 	bits[7] = 0xff & t; t>>=8;
 	bits[6] = 0xff & t; t>>=8;
 	bits[5] = 0xff & t; t>>=8;
-	bits[4] = 0xff & t; t>>=8;
+	bits[4] = 0xff & t;
+	t = count >> (32 - 3);
 	bits[3] = 0xff & t; t>>=8;
 	bits[2] = 0xff & t; t>>=8;
 	bits[1] = 0xff & t; t>>=8;
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  
http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  
http://www.tux.org/lkml/