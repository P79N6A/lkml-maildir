Date: Fri, 06 Feb 2004 20:49:40 -0800
From: "Martin J. Bligh" <>
Subject: Re: [Bugme-new] [Bug 2019] New: Bug from the mm subsystem involving X  (fwd)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2004/2/6/286

--Andi Kleen <ak@suse.de> wrote (on Saturday, February 07, 2004 04:54:03 +0100):
> "Martin J. Bligh" <mbligh@aracnet.com> writes:
> 
>> If we really want to do good testing, we should make a fake NUMA config
>> that can run a 4x SMP box as fake NUMA, with half the memory in each
>> "node" and half the processors ... but I never got around to coding that ;-)
> 
> I have such a patch for x86-64 if anybody is interested in that.
> 
> x86-64 low level NUMA is quite different from IA32 NUMA though so it 
> would be a bit difficult to port.
Not quite sure what you mean ... I was driving at pretending an SMP box
was NUMA ... but the x86_64 is already NUMA ... are you grouping nodes
together into single nodes with 2 cpus each?
What might be intriguing is to use Nick's domains stuff to create a heirarchy
for the scheduler where we have 1 cpu nodes and 2 cpu nodes above that, but
still keep the normal NUMA stuff flat for mem allocation. What might be 
interesting is a heirarchy where if this is the HT connections of cpu layouts:
1 --- 2
|     |
|     | 
|     |
3 --- 4
then domains of (1,2,3) (2,3,4) (1,3,4) (1 2 4), with a view to restricting
the "double hop" traffic as much as possible. But I'm not sure the domains
code copes with multiple overlapping domains - Nick?
Andi, do you already set up the mem allocation fallback zonelists like that?
M.
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  
http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  
http://www.tux.org/lkml/