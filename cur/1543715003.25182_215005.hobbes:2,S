Date: Mon, 19 Apr 2004 18:42:36 +0200
From: Manfred Spraul <>
Subject: Re: slab-alignment-rework.patch in -mc
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-Lkml-Link: https://lkml.org/lkml/2004/4/19/155

Andrea Arcangeli wrote:
>/mirror/kernel/people/akpm/patches/2.6/2.6.5/2.6.5-mc4/broken-out/slab-alignment-rework.patch
>
>I don't think this is right:
>
> 
>
>>-	if (flags & SLAB_HWCACHE_ALIGN) {
>>-		/* Need to adjust size so that objs are cache aligned. */
>>-		/* Small obj size, can get at least two per cache line. */
>>+	if (!align) {
>>+		/* Default alignment: compile time specified l1 cache size.
>>+		 * Except if an object is really small, then squeeze multiple
>>+		 * into one cacheline.
>>+		 */
>>+		align = cache_line_size();
>> 		while (size <= align/2)
>> 			align /= 2;
>>-		size = (size+align-1)&(~(align-1));
>> 	}
>>+	size = ALIGN(size, align);
>> 
>> 
>>
>
>I want anon-vma to really use only 12 bytes, period.
>
Then pass "4" as the align parameter to kmem_cache_create. That's the 
main point of the patch: it's now possible to explicitely specify the 
requested alignment. 32 for the 3rd level page tables, the optimal 
number for the pte_chains, etc.
> No best-guess must
>be made automatically by the slab code, rounding it to 16 bytes.
>
If you pass 0 as align to kmem_cache_create, then it's rounded to L2 
size. It's questionable if that's really the best thing - on 
uniprocessor, 16-byte might result is better performance - there is no 
risk of false sharing.
--
    Manfred
-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  
http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  
http://www.tux.org/lkml/